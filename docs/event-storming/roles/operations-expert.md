# è¿ç»´ä¸“å®¶äº§å‡º - å›¢å»ºåŠ©æ‰‹ (Operations Expert Output - Team Building Assistant)

**è¿ç»´ä¸“å®¶ï¼ˆOPSï¼‰è¾“å‡ºæ–‡æ¡£**

## 1. è¿ç»´ç­–ç•¥æ¦‚è¿° (Operations Strategy Overview)

### 1.1 è§’è‰²å®šä½
è¿ç»´ä¸“å®¶è´Ÿè´£å›¢å»ºåŠ©æ‰‹ç³»ç»Ÿçš„åŸºç¡€è®¾æ–½æ¶æ„ã€éƒ¨ç½²è¿ç»´ã€æ€§èƒ½ç›‘æ§ã€æ•…éšœå¤„ç†å’Œå®‰å…¨ä¿éšœï¼Œç¡®ä¿ç³»ç»Ÿ7Ã—24å°æ—¶ç¨³å®šè¿è¡Œï¼Œæ”¯æŒä¸šåŠ¡å¿«é€Ÿè¿­ä»£æ‰©å±•ã€‚

### 1.2 è¿ç»´ç›®æ ‡å’Œæ ¸å¿ƒåŸåˆ™

```yaml
è¿ç»´ç›®æ ‡ (SREåŸåˆ™):
  å¯ç”¨æ€§: "99.99%ç³»ç»Ÿå¯ç”¨æ€§ï¼Œè®¡åˆ’å†…åœæœºï¼œ4å°æ—¶/æœˆ"
  æ€§èƒ½: "80%APIå“åº”æ—¶é—´ï¼œ200msï¼Œ99%APIå“åº”æ—¶é—´ï¼œ1s"
  å¯è§‚æµ‹æ€§: "100%æœåŠ¡é‡‡é›†ç›‘æ§æŒ‡æ ‡ï¼Œå…³é”®é“¾è·¯å…¨ç¨‹è¿½è¸ª"
  è‡ªåŠ¨åŒ–: "90%è¿ç»´æ“ä½œè‡ªåŠ¨åŒ–ï¼Œéƒ¨ç½²äº¤ä»˜æ—¶é—´ï¼œ30åˆ†é’Ÿ"

 å®‰å…¨æ€§: "é€šè¿‡ç­‰ä¿ä¸‰çº§è®¤è¯ï¼Œ0é«˜é£é™©å®‰å…¨æ¼æ´"

è¿ç»´åŸåˆ™:
  - åŸºç¡€è®¾æ–½å³ä»£ç  (IaC)
  - å¯è§‚æµ‹é©±åŠ¨è¿ç»´ (OEDO)
  - æ•…éšœé¢„é˜²ä¼˜äºæ•…éšœä¿®å¤
  - æŒç»­å­¦ä¹ æ”¹è¿›å¾ªç¯
  - æˆæœ¬æ•ˆç›Šæœ€ä¼˜åŒ–
```

### 1.3 æŠ€æœ¯æ¶æ„è¿ç»´è§†å›¾

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚"
        A[å‰ç«¯: React+Recoil] --> L4[L4è´Ÿè½½å‡è¡¡]
        B[åç«¯: Spring Bootå¾®æœåŠ¡] --> L4[L4è´Ÿè½½å‡è¡¡]
 C[GraphQLç½‘å…³] --> L4[L4è´Ÿè½½å‡è¡¡]
    end

    subgraph "æœåŠ¡ç½‘æ ¼å±‚"
        D[Istio Service Mesh] --> I1[Prometheusç›‘æ§]
        D --> I2[Jaegerè¿½è¸ª]
        D --> I3[Grafanaå‘Šè­¦]
    end

    subgraph "åŸºç¡€æœåŠ¡å±‚"
        E[Kubernetesé›†ç¾¤] --> F[Redisé›†ç¾¤]
        E --> G[PostgreSQLé›†ç¾¤]
 E --> H[Elasticsearché›†ç¾¤]
        E --> I[Message Queue]
    end

    subgraph "åŸºç¡€è®¾æ–½å±‚"
        J[äº‘å¹³å°(AWS/Azure/é˜¿é‡Œäº‘)] --> K[CDNå…¨çƒåŠ é€Ÿ]
        J --> L[è‡ªåŠ¨åŒ–è¿ç»´å·¥å…·]
    J --> M[å¤‡ä»½å®¹ç¾æ–¹æ¡ˆ]
    end

    subgraph "ç›‘æ§è§‚æµ‹å±‚"
     N[ç›‘æ§ç³»ç»Ÿ] --> O[æ—¥å¿—èšåˆ]
        N --> P[æŒ‡æ ‡é‡‡é›†]
        N --> Q[å‘Šè­¦é€šçŸ¥]
    end

    subgraph "å®‰å…¨ç®¡æ§å±‚"
     R[SOCå®‰å…¨è¿è¥] --> S[æ¼æ´æ‰«æ]
        R --> T[è®¿é—®æ§åˆ¶]
        R --> U[å®¡è®¡æŠ¥å‘Š]
    end
```

## 2. åŸºç¡€è®¾æ–½æ¶æ„è®¾è®¡ (Infrastructure Architecture)

### 2.1 é«˜å¯ç”¨æ¶æ„è®¾è®¡

#### 2.1.1 å¤šå¯ç”¨åŒºéƒ¨ç½²æ¶æ„

```yaml
# Kuberneteséƒ¨ç½²é…ç½® - å¤šå¯ç”¨åŒºé«˜å¯ç”¨
apiVersion: v1
kind: Service
metadata:
  name: team-building-api
spec:
  selector:
    app: team-building-api
  ports:
    - name: http
 port: 80
      targetPort: 8080
  type: LoadBalancer

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: team-building-api
  labels:
    app: team-building-api
spec:
  replicas: 6  # 3AZ Ã— 2å‰¯æœ¬ = 6å®ä¾‹
  strategy:
    type: RollingUpdate
    rollingUpdate:
   maxSurge: 1
 maxUnavailable: 0
  selector:
 matchLabels:
      app: team-building-api
  template:
    metadata:
 labels:
        app: team-building-api
      annotations:
prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
    spec:
      # å¤šå¯ç”¨åŒºåäº²å’Œæ€§
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
            matchExpressions:
  - key: app
         operator: In
    values: ["team-building-api"]
     topologyKey: failure-domain.beta.kubernetes.io/zone
     containers:
      - name: api
    image: teambuilding/api:v1.0.0
    ports:
          - containerPort: 8080
  protocol: TCP
        env:
        - name: SPRING_PROFILES_ACTIVE
      value: "production,ops"
      - name: JVM_OPTS
            value: "-Xms512m -Xmx1g -XX:+UseG1GC -XX:MaxGCPauseMillis=200"
        readinessProbe:
       httpGet:
   path: /actuator/health/readiness
        port: 8080
 initialDelaySeconds: 30
periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
        httpGet:
              path: /actuator/health/liveness
  port: 8080
        initialDelaySeconds: 60
    periodSeconds: 30
  timeoutSeconds: 10
        resources:
     requests:
            memory: "512Mi"
       cpu: "500m"
      limits:
        memory: "1Gi"
            cpu: "1000m"
      nodeSelector:
node-type: compute-optimized
 tolerations:
   - key: "compute-intensive"
        operator: "Equal"
   value: "true"
          effect: "NoSchedule"
```

#### 2.1.2 æœåŠ¡ç½‘æ ¼é…ç½®ï¼ˆIstioï¼‰

```yaml
# Istio VirtualServiceé…ç½® - è“ç»¿éƒ¨ç½²
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: team-building-vs
spec:
  hosts:
    - api.team-building.com
  gateways:
    - team-building-gateway
  http:
    - match:
        - headers:
        canary:
        exact: "true"
      route:
     - destination:
     host: team-building-api
          port:
      number: 8080
         subset: v1.1.0
        weight: 100
    - match:
        - headers:     deployment:
  exact: "blue"
      route:
        - destination:
            host: team-building-api
          port:
   number: 8080
        subset: v1.0.0
    weight: 100
    - route:
      - destination:
   host: team-building-api
     port:
       number: 8080
subset: v1.0.0
    weight: 90
        - destination:
   host: team-building-api
        port:
       number: 8080
          subset: v1.1.0
          weight: 10

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: team-building-dr
spec:
  host: team-building-api
  trafficPolicy:
    connectionPool:
      tcp:
 maxConnections: 1000
      http:
    http1MaxPendingRequests: 1000
        maxRequestsPerConnection: 2
        maxRetries: 3
    loadBalancer:
      simple: LEAST_REQUEST
    outlierDetection:
      consecutiveErrors: 5
  interval: 30s
      baseEjectionTime: 30s
maxEjectionPercent: 50
      minHealthPercent: 50
  subsets:
    - name: v1.0.0
labels:
        version: v1.0.0
    - name: v1.1.0
      labels:
 version: v1.1.0
```

### 2.2 æ•°æ®æŒä¹…åŒ–é«˜å¯ç”¨

#### 2.2.1 PostgreSQLé›†ç¾¤æ¶æ„

```yaml
# PostgreSQL Operator - é«˜å¯ç”¨é›†ç¾¤é…ç½®
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: team-building-postgres
spec:
  instances: 3  # 1ä¸»2ä»
  postgres:
    parameters:
      max_connections: "200"
     shared_buffers: "256MB"
      effective_cache_size: "1GB"
      work_mem: "4MB"
      maintenance_work_mem: "64MB"
      wal_buffers: "16MB"
  checkpoint_completion_target: "0.9"
      random_page_cost: "1.1"
    effective_io_concurrency: "200"
  imageName: ghcr.io/cloudnative-pg/postgresql:15.1
  bootstrap:
    initdb:
  database: team_building
      owner: app_user
      secret:
     name: postgres-secret
  postInitSQL:
      - "CREATE EXTENSION IF NOT EXISTS pg_stat_statements;"
      - "CREATE EXTENSION IF NOT EXISTS pgcrypto;"
  - "CREATE EXTENSION IF NOT EXISTS "uuid-ossp";"
  replica:
    enabled: true
    standby: hot_standby_feedback = on
  rollingUpdate:
    enabled: true
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
  memory: "2Gi"
      cpu: "1000m"
  monitoring:
    enabled: true
  backup:
    retentionPolicy: "30d"  # 30å¤©å¤‡ä»½ä¿ç•™
    target: "s3"
    barmanObjectStore:
      destinationPath: "s3://teambuilding-backups/postgres/"
      wal:
  compression: gzip
    data:
        compression: gzip
        			--java.util.regex
  continuous:
    method: "disable"
      encryption: "AES256"
    -- Implemented by
  walBackupFrequencyInSeconds: 300  # 5åˆ†é’ŸWALå¤‡ä»½
  minSyncReplicas: 1
  maxSyncReplicas: 2
  switchoverDelay: 3600  # 1å°æ—¶åˆ‡æ¢å»¶è¿Ÿï¼Œé˜²æ­¢è¯¯åˆ‡æ¢
  --- It detects
  startDelay: 30
  stopDelay: 30
  postInitApplicationSQL:
    - "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_activities_status_date ON activities(status, created_at);"
    - "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_participants_activity_id ON activity_participants(activity_id, status);"
```

#### 2.2.2 Redisé›†ç¾¤é…ç½®

```yaml
# Redis Cluster Configuration
apiVersion: redis.redis.opstreelabs.in/v1beta1
kind: RedisCluster
metadata:
  name: team-building-redis-cluster
spec:
  clusterSize: 6  # 3ä¸»3ä»
  securityContext:
    runAsUser: 1000
    fsGroup: 1000
  kubernetesConfig:
    image: quay.io/opstree/redis:v7.0.8
    imagePullPolicy: IfNotPresent
    resources:
   requests:
        cpu: 100m
    memory: 128Mi
      limits:
 cpu: 500m
    memory: 512Mi
  redisConfig:
    additionalRedisConfig: redis-external-config
  storage:
    volumeClaimTemplate:
   spec:
        accessModes: ["ReadWriteOnce"]
     storageClassName: "fast-ssd"
  resources:
 requests:
   storage: 5Gi
  storage:
    volumeClaimTemplate:
      spec:
   accessModes: ["ReadWriteOnce"]
  storageClassName: "fast-ssd"
resources:
 requests:
            storage: 5Gi
  redisFollower:
    replicas: 3
    affinity:
      podAntiAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
     matchLabels:
  app: redis-cluster
topologyKey: kubernetes.io/hostname
  redisExporter:
    enabled: true
    image: quay.io/opstree/redis-exporter:v1.45.0
    resources:
      requests:
     cpu: 100m
     memory: 128Mi
      limits:
     cpu: 200m
  memory: 256Mi
```

## 3. æŒç»­éƒ¨ç½²ä¸äº¤ä»˜ (CI/CD & DevOps)

### 3.1 ç°ä»£åŒ–CI/CDæµæ°´çº¿

#### 3.1.1 GitLab CI/CDé…ç½®

```yaml
# .gitlab-ci.yml - ä¼ä¸šçº§CI/CDæµæ°´çº¿
stages:
  - æ„å»ºè‡ªæµ‹
  - è´¨é‡é—¨ç¦
  - æµ‹è¯•éªŒè¯
  - éƒ¨ç½²äº¤ä»˜
  - ç›‘æ§éªŒè¯
  - å‘å¸ƒå®Œæˆ

variables:
  DOCKER_REGISTRY: "registry.company.com"
  APP_NAME: "team-building"
  HELM_CHARTS_REPO: "https://helm.company.com/charts"

# æ„å»ºé˜¶æ®µ
docker-build:
 stage: æ„å»ºè‡ªæµ‹
image: docker:24.0.6
  services:
    - docker:24.0.6-dind
  before_script:
 - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $DOCKER_REGISTRY/$APP_NAME:$CI_COMMIT_SHA .
    - docker build -t $DOCKER_REGISTRY/$APP_NAME:latest .
    - docker push $DOCKER_REGISTRY/$APP_NAME:$CI_COMMIT_SHA
    - docker push $DOCKER_REGISTRY/$APP_NAME:latest
  cache:
 paths:
  - target/docker-layers/
 dependencies: []
 except:
    variables:
     - $CI_COMMIT_MESSAGE =~ /\[skip-build\]/

# ä»£ç æ‰«æé˜¶æ®µä»£ç æ‰«æé˜¶æ®µï¼šsonarè´¨é‡å’Œå®‰å…¨æ‰«æ
sonar-quality-gate:
  stage: è´¨é‡é—¨ç¦
  image: sonarsource/sonar-scanner-cli:latest
  script:
    - sonar-scanner
  -Dsonar.projectKey=$APP_NAME
      -Dsonar.projectVersion=$CI_COMMIT_TAG
      -Dsonar.sources=src/
   -Dsonar.coverage.exclusions=**/test/**
        -Dsonar.java.binaries=target/classes
  needs: []
  allow_failure: false
  cache:
policy: pull
    paths:
      - .sonar/cache/

# å®‰å…¨æ‰«æé˜¶æ®µ
security-scan:
  stage: è´¨é‡é—¨ç¦
  image: aquasec/trivy:latest
script:
    - trivy image --severity HIGH,CRITICAL --exit-code 1
      $DOCKER_REGISTRY/$APP_NAME:$CI_COMMIT_SHA
    cache:
policy: pull
  allow_failure: false

# å•å…ƒæµ‹è¯•é˜¶æ®µ
test-unit:
  stage: æµ‹è¯•éªŒè¯
  image: maven:3.9-eclipse-temurin-17
  script:
    - mvn clean test -Dspring.profiles.active=test
    - mvn jacoco:report
    - echo "Test Coverage: $(grep 'Total' target/site/jacoco/index.html | grep -o '[0-9]%')"
artifacts:
    reports:
      junit: target/surefire-reports/TEST-*.xml
      coverage: target/site/jacoco/jacoco.xml
    expire_in: 24h
  coverage: '/Total.*?([0-9]{1,3})%/$/' # æå–è¦†ç›–ç‡æ•°æ®
  allow_failure: true  # å…è®¸å¤±è´¥ä½†ä¸é˜»ç¢åç»­æ­¥éª¤

# é›†æˆæµ‹è¯•é˜¶æ®µ
test-integration:
  stage: æµ‹è¯•éªŒè¯
  image: maven:3.9-eclipse-temurin-17
  services:
    - postgres:15
    - redis:7-alpine
    - moto/moto:latest  # AWSæœåŠ¡æ¨¡æ‹Ÿ
  script:
    - mvn verify -Pintegration -Dspring.profiles.active=integration-test
    - docker-compose up -d  # å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
    - sleep 30  # ç­‰å¾…æœåŠ¡å¯åŠ¨
    - mvn test -Dtest=*IntegrationTest
  artifacts:
    reports:
      junit: target/failsafe-reports/TEST-*.xml
 expire_in: 24h
  needs: ["test-unit"]

# æ€§èƒ½æµ‹è¯•é˜¶æ®µ
test-performance:
 stage: æµ‹è¯•éªŒè¯
  image: loadimpact/k6:latest
  script:
    - k6 run performance-tests/team-building-load.js
- k6 run performance-tests/team-building-stress.js
artifacts:
    reports:
      performance: test-results/performance-report.json
      html: test-results/performance-dashboard.html
 expire_in: 7 days
  only:
    refs:
  - main
  - develop
  - /^release\/.*$/

# é…ç½®ç”Ÿæˆ
config-generate:
  stage: éƒ¨ç½²äº¤ä»˜
  image: alpine/helm:3.12.0
  script:
    - 'helm package helm/team-building --version ${CI_COMMIT_TAG:-"0.0.0"}'
    - 'helm repo index . --url $HELM_CHARTS_REPO'
    - 'curl -T team-building-*.tgz $HELM_CHARTS_REPO'
artifacts:
    paths:
      - team-building-*.tgz
  only:
    refs:
  - tags
    - main
  variables:
 - $CONFIG_GENERATION == "true"
```

#### 3.1.2 Helm Chartså‘å¸ƒç®¡ç†

```yaml
# helm/team-building/Chart.yaml
apiVersion: v2
name: team-building
description: Helm chart for Team Building Assistant application
type: application
version: 1.0.0
appVersion: "1.0.0"
home: https://team-building.company.com
sources:
  - https://github.com/muzhicaomingwang/teambuilding
maintainers:
  - name: Ops Team
    email: ops@company.com

# helm/team-building/values.yaml
replicaCount: 6

image:
  repository: registry.company.com/team-building
  pullPolicy: IfNotPresent
  tag: "1.0.0"

imagePullSecrets:
  - name: registry-secret-name

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
nginx.ingress.kubernetes.io/ssl-redirect: "true"
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
nginx.ingress.kubernetes.io/rate-limit: "100"
 nginx.ingress.kubernetes.io/rate-limit-window: "1m"
  hosts:
  - host: api.team-building.com
    paths:
   - path: /
   pathType: Prefix
        backend:
          service:
  name: team-building
         port:
         number: 80
  tls:
  - secretName: team-building-tls
    hosts:
      - api.team-building.com

autoscaling:
  enabled: true
 minReplicas: 6
maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

resources:
  requests:
    cpu: 500m
memory: 512Mi
  limits:
    cpu: 2
    memory: 2Gi

redis:
  enabled: true
  cluster:
    enabled: true
    slaveCount: 3
    slaveUsage: true

postgresql:
 enabled: true
  global:
    postgresql:
      auth:
        database: team_building
  primary:
    persistence:
      size: 20Gi
      storageClass: fast-ssd
  readReplicas:
 persistence:
  size: 20Gi
      enabled: true
  storageClass: fast-ssd

monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
 labels:
   release: prometheus
    path: /actuator/prometheus

serviceAccount:
  create: true
  annotations:
 eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/team-building
  name: team-building-service-account
 ```

### 3.2 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

#### 3.2.1 ç¯å¢ƒå‡†å¤‡è„šæœ¬

```bash
#!/bin/bash
# setup-production-env.sh - ç”Ÿäº§ç¯å¢ƒåˆå§‹åŒ–è„šæœ¬

set -euo pipefail

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥å¿…è¦æ¡ä»¶
check_prerequisites() {
    log_info "æ£€æŸ¥å‰ç½®æ¡ä»¶..."

    # æ£€æŸ¥kubectl
    if ! command -v kubectl > /dev/null 2>&1; then
    log_error "kubectlæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…kubectl"
  exit 1
    fi

    # æ£€æŸ¥helm
    if ! command -v helm > /dev/null 2>&1; then
        log_error "helmæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…helm"
        exit 1
    fi

    # æ£€æŸ¥é›†ç¾¤è¿æ¥
    if ! kubectl cluster-info > /dev/null 2>&1; then
   log_error "æ— æ³•è¿æ¥åˆ°Kubernetesé›†ç¾¤ï¼Œè¯·æ£€æŸ¥kubeconfigé…ç½®"
        exit 1
    fi

    log_info "âœ“ æ‰€æœ‰å‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡"
}

# åˆ›å»ºå‘½åç©ºé—´
create_namespace() {
   kubectl apply -f -
EOF << EOF
apiVersion: v1
kind: Namespace
metadata:
  name: team-building
  labels:
 environment: production
    app: team-building
EOF

    log_info "âœ“ å‘½åç©ºé—´åˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºåŸºç¡€æœåŠ¡
setup_basic_services() {
    log_info "åˆ›å»ºåŸºç¡€æœåŠ¡..."

    # åˆ›å»ºæœåŠ¡è´¦å·
    kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
 name: team-building
  namespace: team-building
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: team-building-role
rules:
- apiGroups: [""]
   resources: ["pods", "services", "endpoints"]
   verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
    verbs: ["get", "list", "watch", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: team-building-binding
subjects:
- kind: ServiceAccount
  name: team-building
  namespace: team-building
roleRef:
  kind: ClusterRole
  name: team-building-role
 apiGroup: rbac.authorization.k8s.io
EOF

    log_info "âœ“ åŸºç¡€æœåŠ¡åˆ›å»ºå®Œæˆ"
}

# é…ç½®å­˜å‚¨
deploy_storage() {
    log_info "éƒ¨ç½²å­˜å‚¨ç³»ç»Ÿ..."

    # åˆ›å»ºStorageClassï¼ˆé’ˆå¯¹ä¸åŒçš„æ€§èƒ½éœ€æ±‚ï¼‰
    kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
throughput: "125"
  encrypted: "true"
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
    type: gp3
  encrypted: "true"
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
EOF

    log_info "âœ“ å­˜å‚¨ç³»ç»Ÿé…ç½®å®Œæˆ"
}

# éƒ¨ç½²ç›‘æ§stack
deploy_monitoring() {
    log_info "éƒ¨ç½²ç›‘æ§ç³»ç»Ÿ..."

    # Prometheus
    helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
 --namespace team-building \
    --create-namespace \
     --set prometheus.prometheusSpec.retention=30d \
 --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName=fast-ssd \
        --set grafana.persistence.enabled=true \
--set grafana.persistence.storageClassName=fast-ssd \
        --set grafana.persistence.size=50Gi \
        --wait

    # åˆ›å»ºServiceMonitor
    kubectl apply -f - <<EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: team-building-monitor
  namespace: team-building
spec:
  selector:
    matchLabels:
   app: team-building
  endpoints:
  - port: metrics
    interval: 30s
    path: /actuator/prometheus
EOF

    log_info "âœ“ ç›‘æ§ç³»ç»Ÿéƒ¨ç½²å®Œæˆ"
}

# ä¸»æ‰§è¡Œå‡½æ•°
main() {
    log_info "å¼€å§‹è®¾ç½®å›¢å»ºåŠ©æ‰‹ç”Ÿäº§ç¯å¢ƒ..."

  check_prerequisites
    create_namespace
    setup_basic_services
    deploy_storage()
    deploy_monitoring

    log_info "ğŸ‰ ç”Ÿäº§ç¯å¢ƒåŸºç¡€è®¾æ–½è®¾ç½®å®Œæˆï¼"
    log_info "ä¸‹ä¸€æ­¥: è¯·é…ç½®æ•°æ®åº“è¿æ¥å’Œéƒ¨ç½²åº”ç”¨ç¨‹åº"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

#### 3.2.2 é›†ç¾¤å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# health-check.sh - é›†ç¾¤å¥åº·æ£€æŸ¥è„šæœ¬

components=(
    "api:deployment/team-building-api"
    "postgres:cluster/postgres"
    "redis:cluster/redis"
    "ingress:service/team-building-ingress-controller"
    "prometheus:deployment/prometheus-kube-prometheus-stack-prometheus"
)

function check_component() {
    local comp_name=$1
    local comp_type=$2

    echo "æ£€æŸ¥ $comp_name çŠ¶æ€..."

    case $comp_type in
        deployment/*)
  ready=$(kubectl get deployment ${comp_type#deployment/} -n team-building -o jsonpath='{.status.readyReplicas}')
  desired=$(kubectl get deployment ${comp_type#deployment/} -n team-building -o jsonpath='{.status.replicas}')
    if [[ $ready == $desired && $ready -gt 0 ]]; then
          echo "âœ“ $comp_name: å¥åº· (Ready: $ready/$desired)"
       else
          echo "âœ— $comp_name: å¼‚å¸¸ (Ready: $ready/$desired)"
            return 1
     fi
  ;;
      cluster/*)
     ready=$(kubectl get ${comp_type#*/} -n team-building -o jsonpath='{.status.readyInstances}')
   total=$(kubectl get ${comp_type#*/} -n team-building -o jsonpath='{.status.instances}')
            if [[ $ready == $total && $ready -gt 0 ]]; then
      echo "âœ“ $comp_name: å¥åº· (Ready: $ready/$ready)"
       else
     echo "âœ— $comp_name: å¼‚å¸¸ (Ready: $ready/$total)"
    return 1
        fi
  ;;
        *)
            status=$(kubectl get $comp_type -n team-building -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  if [[ -n $status ]]; then
          echo "âœ“ $comp_name: å¥åº· (IP: $status)"
       else
  echo "âœ— $comp_name: å¼‚å¸¸ (æ— IP)"
          return 1
        fi
   ;;
    esac
}

function check_resources() {
    echo ""
    echo "é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µæ£€æŸ¥..."

    # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
    kubectl top nodes -n team-building
    kubectl top pods -n team-building | head -20

    # å­˜å‚¨ä½¿ç”¨æƒ…å†µ
    kubectl get pvc -n team-building -o wide
}

function check_logs() {
  echo ""
    echo "æœ€è¿‘å¼‚å¸¸æ—¥å¿—æ£€æŸ¥..."

  # æ£€æŸ¥æœ€è¿‘1å°æ—¶çš„é”™è¯¯æ—¥å¿—
kubectl logs -l app=team-building-api -n team-building --since=1h |
        grep -i "error\|exception\|failed" |
        tail -20 || echo "æ— æœ€è¿‘é”™è¯¯æ—¥å¿—"
}

# ä¸»å‡½æ•°
main() {
    echo "ğŸš€ å¼€å§‹å›¢å»ºåŠ©æ‰‹ç³»ç»Ÿå¥åº·æ£€æŸ¥..."

    healthy=0
    total=${#components[@]}

    for comp in "${components[@]}"; do
   IFS=':' read -r comp_name comp_type <<< "$comp"
        check_component "$comp_name" "$comp_type" || ((healthy++))
    done

 check_resources
  check_logs

    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "å¥åº·æ£€æŸ¥ç»“æœæ€»ç»“"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
 echo "âœ“ å¥åº·ç»„ä»¶: $((healthy - ${#components[@]}))/$healthy"

    if [ $healthy -eq $total ]; then
        echo "ğŸ‰ ç³»ç»ŸçŠ¶æ€ï¼šå…¨éƒ¨å¥åº·"
    elif [ $healthy -ge $((total*4/5)) ]; then
        echo "âš ï¸  ç³»ç»ŸçŠ¶æ€ï¼šè½»å¾®å¼‚å¸¸ï¼ˆå»ºè®®æ£€æŸ¥ï¼‰"
    else
     echo "ğŸš¨ ç³»ç»ŸçŠ¶æ€ï¼šéœ€è¦ç«‹å³å…³æ³¨ï¼ˆä¸¥é‡ï¼‰"
        echo "å»ºè®®è¿è¡Œï¼škubectl get events -n team-building --sort-by='.lastTimestamp'"
    fi
}

main "$@"
```

## 4. æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ– (Performance Monitoring & Optimization)

### 4.1 å…¨æ ˆå¯è§‚æµ‹æ€§è®¾è®¡

#### 4.1.1 å¤šç»´åº¦ç›‘æ§ä½“ç³»

```mermaid
graph LR
    subgraph "Metrics Collection"
        A[App Metrics] --> P[Prometheus]
        B[System Metrics] --> P
        C[Business Metrics] --> P
        D[Custom Metrics] --> P
    end

    subgraph "Logging"
        E[App Logs] --> L[Loki]
     F[System Logs] --> L
        G[Audit Logs] --> L
    end

    subgraph "Tracing"
    H[Request Traces] --> J[Jaeger]
        I[Database Queries] --> J
        K[Service Mesh] --> J
    end

    subgraph "Analysis"
 P --> D[Dashboards]
        L --> D
   J --> D
        P --> A[Analytics]
        L --> A
      J --> A
    end
```

#### 4.1.2 ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§å®ç°

```java
/**
 * ä¸šåŠ¡å…³é”®æŒ‡æ ‡é‡‡é›†å™¨
 */
@Component
@Slf4j
@RequiredArgsConstructor
public class BusinessMetricsCollector {

    private final MeterRegistry meterRegistry;
    private final ActivityService activityService;
    private final TeamService teamService;
    private final SubscriptionService subscriptionService;

    /**
     * æ ¸å¿ƒä¸šåŠ¡KPIæŒ‡æ ‡
     */
    @Scheduled(fixedDelay = 300_000) // 5åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
    public void collectBusinessKPIs() {
        Instant now = Instant.now();

        // æœˆåº¦æ´»è·ƒåº¦
    gaugeMetric("business.monthly_active_teams",
         () -> teamService.getMonthlyActiveTeamsCount(now),
            toString(yearMonth));

        // æ´»åŠ¨åˆ›å»ºæˆåŠŸç‡(30å¤©å†…)
        double successRate = calculateActivityCreationSuccessRate(now);
        gaugeMetric("business.activity_creation_success_rate",
          () -> successRate,
   Tags.of("period", "30d"));

   // å¹³å‡å›¢å»ºé¢‘ç‡
   double avgFrequency = calculateEmployeeTeamBuildingFrequency(now);
 gaugeMetric("business.employee_team_building_frequency",
     () -> avgFrequency,
    Tags.of("unit", "activities_per_year"));

        // AIæ¨èé‡‡çº³ç‡
  double aiAdoptionRate = calculateAIRecommendationAdoptionRate(now);
        gaugeMetric("business.ai_recommendation_adoption_rate",
      () -> aiAdoptionRate,
        Tags.of("metric", "recommendation"));

     // ROIè®¡ç®—
        double roi = calculateTeamBuildingROI(YearMonth.from(now));
      gaugeMetric("business.team_building_roi_percent",
      () -> roi,
   Tags.of("period", "monthly"));
    }

    /**
     * æŠ€æœ¯æ ˆæ€§èƒ½æŒ‡æ ‡
     */
    @Scheduled(fixedDelay = 60_000) // 1åˆ†é’Ÿæ›´æ–°
    public void collectSystemMetrics() {
    // JVMæŒ‡æ ‡
  gaugeMetric("jvm.memory.used",
            () -> ManagementFactory.getMemoryMXBean().getHeapMemoryUsage().getUsed(),
            Tags.of("type", "heap"));

        // æ•°æ®åº“è¿æ¥æ± çŠ¶æ€
HikariDataSource dataSource = (HikariDataSource) this.dataSource;
        gaugeMetric("db.connection_pool.active",
     () -> dataSource.getHikariPoolMXBean().getActiveConnections(),
            Tags.of("pool", "hikari"));

        // Redisé›†ç¾¤çŠ¶æ€
        gaugeMetric("redis.cluster.nodes.online",
          () -> redisTemplate.getConnectionFactory().getClusterConnection().clusterGetNodes().stream()
  .filter(node -> node.isConnected())
   .count());
}

    /**
     * ç”¨æˆ·è¡Œä¸ºåˆ†ææŒ‡æ ‡
     */
@EventListener
 public void handleUserActivity(ActivityCreatedEvent event) {
  // ç”¨æˆ·æ´»è·ƒåº¦ç»Ÿè®¡
     counterMetric("user.activity.type.created",
         Tags.of("user_type", event.getUserType()));

        // åŠŸèƒ½ä½¿ç”¨é¢‘ç‡ç»Ÿè®¡
        incrementCounter("feature.usage.create_activity",
      Tags.of("activity_type", event.getActivityType()));
    }

    /**
     * æ€§èƒ½åŸºå‡†æµ‹è¯•
     */
    @Timed(value = "business.performance.ai_recommendation", description = "AIæ¨èç”Ÿæˆæ—¶é—´")
    public List<Recommendation> generateRecommendations(RecommendationRequest request) {
   // ä¸šåŠ¡é€»è¾‘...
    }
}
```

#### 4.1.3 å¤šå±‚çº§å‘Šè­¦è®¾è®¡

```yaml
# PrometheusRule - åˆ†å±‚å‘Šè­¦è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: team-building-alerts
  namespace: team-building
spec:
  groups:
  - name: critical_alerts
 interval: 30s
    rules:
 # P0çº§å‘Šè­¦
    - alert: TeamBuildingAPIHighErrorRate
        expr: |
          (sum(rate(http_requests_total{service="team-building-api",status=~"5.."}[5m]))
          / sum(rate(http_requests_total{service="team-building-api"}[5m]))) > 0.05
        for: 2m
      labels:
    severity: critical
      priority: P0
     team: platform-sre
   annotations:
          summary: "å›¢å»ºåŠ©æ‰‹APIé”™è¯¯ç‡è¿‡é«˜"
      description: "IPé”™è¯¯ç‡ {{ $value | humanizePercentage }}ï¼ŒæŒç»­æ—¶é—´{{ $labels.duration }}"
        runbook_url: "https://wiki.company.com/runbooks/api-high-error-rate"
        - alert: DatabaseConnectionPoolExhausted
    expr: db_connection_pool_active_connections > db_connection_pool_max_connections * 0.95
      for: 1m
        labels:
          severity: critical
      priority: P0
        annotations:
          summary: "æ•°æ®åº“è¿æ¥æ± å³å°†è€—å°½"
      description: "è¿æ¥æ± ä½¿ç”¨ç‡ {{ $value | humanizePercentage }}ï¼Œå½“å‰æ´»è·ƒè¿æ¥{{ $labels.active }}"

  - name: business_alerts
    interval: 60s
    rules:
      # ä¸šåŠ¡çº§å‘Šè­¦
    - alert: TeamBuildingBusinessErrorRate
        expr: |
          (sum(rate(business_error_total{service="team-building"}[10m]))
          / sum(rate(business_operations_total{service="team-building"}[10m]))) > 0.02
      for: 5m
        labels:
     severity: warning
          priority: P1
          business_impact: true
        annotations:
    summary: "å›¢å»ºåŠ©æ‰‹ä¸šåŠ¡è¯·æ±‚é”™è¯¯ç‡ä¸Šå‡"
  description: "å›¢å»ºåŠ©æ‰‹ä¸šåŠ¡è¯·æ±‚é”™è¯¯ç‡: {{ $value | humanizePercentage }}"

    - alert: LowTeamBuildingActivityCreationSuccessRate
        expr: |
          (sum(rate(activity_creation_success_total{service="team-building"}[1h]))
 / sum(rate(activity_creation_attempts_total{target="team-building"}[1h]))) < 0.95
        for: 15m
        labels:
 severity: warning
     priority: P2
        annotations:
      summary: "å›¢å»ºæ´»åŠ¨åˆ›å»ºæˆåŠŸç‡åä½"
          description: "è¿‡å»1å°æ—¶æ´»åŠ¨åˆ›å»ºæˆåŠŸç‡: {{ $value | humanizePercentage }}"

  - name: performance_alerts
    interval: 120s
    rules:
      # æ€§èƒ½ç›¸å…³å‘Šè­¦
 - alert: HighAPIResponseTime
 expr: |
          histogram_quantile(0.99,
      sum(rate(http_request_duration_seconds_bucket{service="target="team-building-api"}[5m])) by (le)) > 1
     for: 5m
      labels:
        severity: warning
        priority: P2
    annotations:
   summary: "APIå“åº”æ—¶é—´P99è¿‡é«˜"
          description: "IP P99å“åº”æ—¶é—´: {{ $value }}s"

        - alert: HighMemoryUsage
    expr: |
   (container_memory_working_set_bytes{pod=~"team-building-.*"}
    / container_spec_memory_limit_bytes{pod=~"team-building-.*"}) > 0.9
        for: 10m
        labels:
          severity: warning
 priority: P2
 annotations:
            summary: "æœåŠ¡å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
            description: "å†…å­˜ä½¿ç”¨ç‡: {{ $value | humanizePercentage }}"

  - name: capacity_alerts
  interval: 300s
    rules:
  # å®¹é‡è§„åˆ’å‘Šè­¦
      - alert: HighCPUUsageForecast
        expr: |
       predict_linear(cpu_usage_percent{service="team-building"}[1h], 3600*24) > 80
    labels:
   severity: info
          priority: P3
 annotations:
      summary: "CPUä½¿ç”¨ç‡24å°æ—¶é¢„æµ‹å°†è¶…80%"
       description: "æ ¹æ®çº¿æ€§é¢„æµ‹æ¨¡å‹ï¼ŒCPUä½¿ç”¨ç‡å°†è¶…è¿‡80%"
```

### 4.2 å®¹é‡è§„åˆ’ä¸æˆæœ¬ä¼˜åŒ–

#### 4.2.1 è‡ªåŠ¨åŒ–æ‰©å®¹ç­–ç•¥

```yaml
# HorizontalPodAutoscaler - åŸºäºä¸šåŠ¡æŒ‡æ ‡æ‰©å®¹
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: team-building-api-hpa
  namespace: team-building
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: team-building-api
  minReplicas: 6
  maxReplicas: 50
  metrics:
    # CPUä½¿ç”¨ (50%é˜ˆå€¼)
    - type: Resource
      resource:
        name: cpu
        target:
    type: Utilization
          averageUtilization: 50
    # å†…å­˜ä½¿ç”¨ (60%é˜ˆå€¼)
    - type: Resource
   resource:
      name: memory
        target:
          type: Utilization
          averageUtilization: 60
    # è‡ªå®šä¹‰æŒ‡æ ‡ (QPS)
  - type: Pods
      pods:
    metric:
          name: http_requests_per_second
      target:
  type: AverageValue
     averageValue: "100"
    # ä¸šåŠ¡æŒ‡æ ‡ (å¹¶å‘ç”¨æˆ·æ•°)
    - type: Pods
      pods:
        metric:
  name: active_team_building_sessions
          target:
       type: AverageValue
         averageValue: "500"
  behavior:
    # æ‰©å®¹ç­–ç•¥
    scaleUp:
      stabilizationWindowSeconds: 60  # 1åˆ†é’Ÿå†…ä¿æŒç¨³å®šå†æ‰©å®¹
      policies:
        - type: Percent
    value: 50  # æ¯æ¬¡æ‰©å®¹50%
 periodSeconds: 30
        - type: Pods
          value: 4  # æœ€å°‘æ‰©å®¹4Pod
 periodSeconds: 60
  selectPolicy: Min # é€‰æ‹©æœ€ä¿å®ˆçš„æ‰©å®¹ç­–ç•¥
    # ç¼©å®¹ç­–ç•¥
    scaleDown:
      stabilizationWindowSeconds: 300  # 5åˆ†é’Ÿå†…ä¿æŒç¨³å®šå†ç¼©å®¹
      policies:
  - type: Percent
        value: 10    # æ¯æ¬¡ç¼©å®¹10%
     periodSeconds: 60
        - type: Pods
        value: 1 # æœ€å°‘ç¼©å®¹1Pod
       periodSeconds: 60
        selectPolicy: Min  # é€‰æ‹©æœ€ä¿å®ˆçš„ç¼©å®¹ç­–ç•¥
```

#### 4.2.2 æˆæœ¬ä¼˜åŒ–åˆ†æ

```python
#!/usr/bin/env python3
# cost_optimizer.py - æˆæœ¬ä¼˜åŒ–åˆ†æè„šæœ¬

import boto3
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

class CloudCostOptimizer:
    def __init__(self, service, region):\n        self.ce_client = boto3.client('ce', region_name=region)
        self.instance_types = {
    'compute': ['c6i.large', 'c6i.xlarge', 'c6i.2xlarge'],
      'memory': ['r6i.large', 'r6i.xlarge', 'r6i.2xlarge'],
     'general': ['m6i.large', 'm6i.xlarge', 'm6i.2xlarge']
        }

    def analyze_current_costs(self, start_date, end_date):
 """åˆ†æå½“å‰æˆæœ¬ç»“æ„"""

        response = self.ce_client.get_cost_and_usage(
     TimePeriod={
     'Start': start_date.strftime('%Y-%m-%d'),
      'End': end_date.strftime('%Y-%m-%d')
            },
     ...","Oh"
      Granularity='MONTHLY',
  Metrics=['BlendedCost', 'UnblendedCost', 'UsageQuantity'],
  GroupBy=[
       {
         'Type': 'DIMENSION',
            'Key': 'SERVICE'
   },
      {
        'Type': 'TAG',
                'Key': 'Environment'
 }
      ]
        )

        # å›¢é˜Ÿå»ºè®¾åº”ç”¨ç›¸å…³çš„æˆæœ¬è§£æ
        team_building_costs = [
   cost for cost in response['ResultsByTime']
   if any('team-building' in group['Keys'] for group in cost['Groups'])
        ]

        return self.parse_costs(team_building_costs)

     def calculate_savings_potential(self, baseline_data):
        """è®¡ç®—èŠ‚çœæ½œåŠ›"""
    recommendations = []

        # 1. å®ä¾‹ç±»å‹ä¼˜åŒ–
    current_instance_cost = baseline_data['compute_instances']
 savings = self.analyze_instance_optimization(current_instance_cost)
    recommendations.append(savings)

  # 2. Reserved Instancesåˆ†æ
ri_usage = baseline_data['on_demand_instances']
        ri_savings = self.analyze_reserved_instance_savings(ri_usage)
   recommendations.append(ri_savings)

        # 3. å­˜å‚¨ä¼˜åŒ–
        storage_cost = baseline_data['storage']
     storage_savings = self.analyze_storage_optimization(storage_cost)
   recommendations.append(storage_savings)

        # 4. ç½‘ç»œä¼˜åŒ–
      data_transfer_cost = baseline_data['data_transfer']
        network_savings = self.analyze_network_optimization(data_transfer_cost)
        recommendations.append(network_savings)

        return recommendations

    def generate_cost_optimization_report(self):
    """ç”Ÿæˆæˆæœ¬ä¼˜åŒ–å»ºè®®æŠ¥å‘Š"""
        # è·å–æœ€è¿‘3ä¸ªæœˆçš„æ•°æ®\n     end_date = datetime.now()
        start_date = end_date - timedelta(dayss=90)

   baseline = self.analyze_current_costs(start_date, end_date)
   recommendations = self.calculate_savings_potential(baseline)

        # è®¡ç®—æ¯ä¸ªäº‘æœåŠ¡æä¾›å•†çš„ä¼˜åŒ–æ–¹æ¡ˆ
    optimization_scenarios = []

        for scenario in [{'name': 'Conservative', 'savings': 0.15},
  {'name': 'Optimized', 'savings': 0.30},
  {'name': 'Aggressive', 'savings': 0.45}]:
          optimized_scenario = {
   'scenario': scenario['name'],
     'estimated_savings_percent': scenario['savings'],
       'estimated_monthly_savings': baseline['total'] * scenario['savings'],
     'implementation_priority': self.get_implementation_priority(scenario),
        'timeline': self.get_optimization_timeline(scenario),
     'risks': self.assess_optimization_risks(scenario)
           }
   optimization_scenarios.append(optimized_scenario)

    # è€ƒè™‘å¿ è¯šåº¦è®¡åˆ’å’Œé•¿æœŸæ‰¿è¯º
        loyalty_analysis = self.analyze_elasticsearch_loyalty_programs()

        return {
   'baseline_analysis': baseline,
    'savings_recommendations': recommendations,
            'optimization_scenarios': optimization_scenarios,
      'loyalty_analysis': loyalty_analysis,
            'total_potential_savings': sum(r['savings_12mo'] for r in recommendations),
     'report_date': datetime.now()
        }

    def recommend_instance_types(self, current_usage):
        """åˆ†æå¹¶æä¾›å®ä¾‹ç±»å‹æ¨è"""
     recommendations = {
 'current': {
            'instance': current_usage['instance_type'],
            'monthly_cost': current_usage['monthly_cost'],
            'cpu_utilization': current_usage['cpu_utilization'],
       'memory_utilization': current_usage['memory_utilization']
        },
   'recommendations': []
        }

        # åŸºäºä½¿ç”¨ç‡æ¨èä¼˜åŒ–å®ä¾‹ç±»å‹
     if current_usage['cpu_utilization'] < 30 and current_usage['memory_utilization'] < 40:
            # ä½ä½¿ç”¨ç‡ï¼Œå»ºè®®ä½¿ç”¨æ›´å°å‹å®ä¾‹
     recommendations['recommendations'].append({
      'instance_type': 'c6i.large',
     'cpu': 2,
     'memory': '4GiB',
    'est_savings': '60%',
   'confidence': '95%'
  })

  elif current_usage['cpu_utilization'] > 80 and current_usage['memory_utilization'] < 60:
         # CPUå¯†é›†å‹åº”ç”¨ï¼Œå»ºè®®ä½¿ç”¨è®¡ç®—ä¼˜åŒ–å‹
    recommendations['recommendations'].append({
           'instance_type': 'c6i.xlarge',
                'cpu': 4,
             'memory': '8GiB',
    'est_savings': '20%',
    'confidence': '90%'
        })

    return recommendations

def main():
    """ä¸»å‡½æ•°"""
    optimizer = CloudCostOptimizer('AWS', 'us-west-2')
  report = optimizer.generate_cost_optimization_report()

    print("ğŸš€ å›¢å»ºåŠ©æ‰‹æˆæœ¬ä¼˜åŒ–åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    print(f"åˆ†æå‘¨æœŸ: æœ€è¿‘90å¤© (è‡³{report['report_date']})")
    \n    print(f"\nğŸ’° å½“å‰æœˆåº¦æˆæœ¬: ${report['baseline_analysis']['monthly_cost']:,.2f}")
    print(f"\nğŸ¯ æ½œåœ¨èŠ‚çœé¢åº¦: ${report['total_potential_savings']:,.2f}/å¹´")
   print(f"\nèŠ‚çœç™¾åˆ†æ¯”: {report['total_potential_savings']/report['baseline_analysis']['monthly_cost']/12*100:.1f}%")

    print("\nğŸ“‹ è¯¦ç»†å»ºè®®ä¸å®æ–½æ–¹æ¡ˆ")
    for rec in report['savings_recommendations']:
        print(f"  - {rec['type']}: èŠ‚çœ ${rec['savings_12mo']:,.2f}/å¹´")
      print(f"    ROI: {rec['roi']}x, å®ç°éš¾åº¦: {rec['implementation_difficulty']}")

    print("\n" + "=" * 60)
    print("âœ… åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè¯·æŸ¥çœ‹å®Œæ•´æŠ¥å‘Šè·å–è¯¦ç»†å®æ–½æ–¹æ¡ˆ")

if __name__ == "__main__":
    main()
```

## 5. å®‰å…¨è¿ç»´ç®¡ç† (Security Operations)

### 5.1 å®‰å…¨è¿ç»´ç­–ç•¥

#### 5.1.1 å¤šå±‚å®‰å…¨é˜²å¾¡ä½“ç³»

```yaml
# Security Policy - å®‰å…¨è¿è¥ç­–ç•¥æ–‡æ¡£
apiVersion: v1
kind: SecurityPolicy
docType: OperationsManual
metadata:
  name: team-building-security-ops
  version: "1.0"
spec:
  strategy:
    defense_in_depth:   # çºµæ·±é˜²å¾¡
        - layer: "Network Perimeter"
     description: "WAF + CDN + DDoS Protection"
components: ["CloudFlare WAF", "AWS Shield Standard"]
        - layer: "Application Security"
      description: "OAuth2 + JWT + RBAC"
          components: ["Spring Security", "Auth0"]
  - layer: "Infrastructure Security"
description: "Kubernetes Security + Container Scanning"
  components: ["Pod Security Standards", "Falco"]
        - layer: "Data Security"
          description: "Encryption + Access Control"
          components: ["Key Management Service", "Database Encryption"]
        - layer: "Monitoring & Response"
description: "SOC + SIEM + IR"
     components: ["Splunk", "PagerDuty", "JIRA Service Desk"]

    zero_trust: true
    compliance: ["SOC2", "ISO27001", "GDPR"]
    threat_modeling: continuous

  network_security:
    segmentation: true
    microsegmentation: kubernetes_network_policies
    encryption: all_traffic_encrypted
    key_rotation: 90_days

  application_security:
    sast: true
        sca: true
        dast: true
    container_scanning: true
  runtime_protection: true

  identity_access:
    mfa_required: true
    password_policy: enterprise_strong
    session_management: tokens_with_rotation
    privilege_access_management: true

  data_protection:
    classification: true
encryption_at_rest: true
    encryption_in_transit: true
    key_lifecycle: automated

  monitoring_security:
    log_retention: "365_days_security"
    anomaly_detection: ml_based
  response_sla:
    p1: "15_minutes"
    p2: "1_hour"
 p3: "4_hours"
    p4: "24_hours"

  incident_response:
    team: security-team
  playbook: "security-incident-response"
  escalation: "auto"
```

#### 5.1.2 IAMå’Œè®¿é—®æ§åˆ¶

```json
{
  "version": "2023-11-28",
  "Statement": [
    {
      "Sid": "TeamBuildingAdminAccess",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:role/TeamBuildingAdmin"
      },
  "Action": [
 "teambuilding:*"
      ],
      "Resource": "arn:aws:teambuilding:us-west-2:123456789012:*",
      "Condition": {
        "StringEquals": {
          "teambuilding:RequestedRegion": "us-west-2"
        },
        "IpAddress": {
          "aws:SourceIp": [
    "10.0.0.0/8",
     "172.16.0.0/12"
          ]
        }
      }
    },
    {
      "Sid": "TeamBuildingDeveloperAccess",
      "Effect": "Allow",
      "Principal": {
     "AWS": "arn:aws:iam::123456789012:role/TeamBuildingDeveloper"
      },
      "Action": [
     "teambuilding:Read*",
    "teambuilding:CreateActivity",
        "teambuilding:UpdateActivity",
        "teambuilding:Query*"
      ],
      "Resource": "arn:aws:teambuilding:us-west-2:123456789012:activity/*",
      "Condition": {
StringEquals": {
          "teambuilding:RequestTag/Department": "${aws:PrincipalTag/Department}"
    },
        "DateGreaterThan": {
          "aws:CurrentTime": "2023-01-01T00:00:00Z"
        },
     "DateLessThan": {
       "aws:CurrentTime": "2025-12-31T23:59:59Z"
 }      }
    }
  ]
}
```

#### 5.1.3 å®‰å…¨äº‹ä»¶å“åº”è‡ªåŠ¨åŒ–

```python
#!/usr/bin/env python3
# security_connector.py - å®‰å…¨äº‹ä»¶å“åº”è¿æ¥è„š

import json
import boto3
import requests
from datetime import datetime
from typing import Dict, List

class SecurityIncidentHandler:
    def __init__(self):
        self.security_hub = boto3.client('securityhub')
   self.guard_duty = boto3.client('guardduty')
        self.pagerduty = PagerDutyAPI()
  self.slack = SlackAPI()
        self.splunk = SplunkAPI()

    def process_security_alert(self, alert_data: Dict):
   """å¤„ç†å®‰å…¨å‘Šè­¦æ•°æ®"""

      # 1. è‡ªåŠ¨å¨èƒçº§åˆ«åˆ†ç±»
        severity = self.classify_security_threat(alert_data)

        # 2. è‡ªåŠ¨å“åº”çº§åˆ«é€‰æ‹©
        playbook = self.select_response_playbook(alert_data, severity)

        # 3. è‡ªåŠ¨å“åº”æ‰§è¡Œ
        response_result = self.execute_response(playbook, alert_data)

        # 4. å·¥ä½œæµè‡ªåŠ¨åŒ–
      workflow = self.create_incident_workflow(alert_data, severity)
        self.execute_workflow(workflow)

  # 5. é€šçŸ¥ç›¸å…³å›¢é˜Ÿ
        self.notify_response_teams(alert_data, severity, response_result)

        return response_result

    def classify_security_threat(self, alert: Dict) -> str:
        """æ™ºèƒ½å¨èƒçº§åˆ«åˆ†ç±» æ™ºèƒ½å¨èƒçº§åˆ«åˆ†ç±»"""

        threat_level = "LOW"

   # CVSSè¯„åˆ†
      if alert.get('cvss_score', 0):
         if alert['cvss_score'] >= 9.0:
          threat_level = "CRITICAL"
            elif alert['cvss_score'] >= 7.0:
       threat_level = "HIGH"
     elif alert['cvss_score'] >= 4.0:
threat_level = "MEDIUM"

        # ä¸šåŠ¡å½±å“åˆ†æ
        if self.analyze_business_impact(alert) == "HIGH":
            threat_level = max(threat_level, "HIGH")

  # ç´§è¿«æ€§åˆ†æ
    if alert.get('urgency_score', 0) >= 8.0:
        threat_level = max(threat_level, "HIGH")

        return threat_level

    def select_response_playbook(self, alert: Dict, severity: str) -> Dict:
        """é€‰æ‹©é¢„å®šä¹‰çš„å“åº”æ“ä½œæ‰‹å†Œ é€‰æ‹©é¢„å®šä¹‰çš„å“åº”æ“ä½œæ‰‹å†Œ"""

   playbook_templates = {
       "CRITICAL": {
       "immediate_actions"
: [
            "block_affected_ips",
       "isolate_affected_systems",
        "collect_forensic_artifacts",
     "notify_security_team"
            ],
     "team_notification": "security-team",
            "escalation_sla": "15_minutes",
 "auto_response": True
        },
        "HIGH": {
   "immediate_actions": [
                "collect_detailed_logs",
  "enable_extended_monitoring",
        "schedule_security_review"
            ],
   "team_notification": "platform-team",
            "escalation_sla": "1_hour",
     "auto_response": True
      },
   "MEDIUM": {
        "immediate_actions": [
      "collect_standard_logs"
            ],
      team_notification": "developer-team",
         "escalation_sla": "4_hours",
            "auto_response": False
        },
        "LOW": {
            "immediate_actions": [],
         "team_notification": "app-team",
 ``          "escalation_sla": "24_hours",escalation_sla": "24_hours",
       "auto_response": False
        }
    }

        # æ ¹æ®å‘Šè­¦ç±»å‹å®šåˆ¶å“åº”ç­–ç•¥
        customized_playbook = playbook_templates[severity].copy()

        if alert.get('type') == 'DATABASE_LEAK':
   customized_playbook['immediate_actions'].extend([
          "rotate_database_credentials",
       "enable_connection_encryption",
    "review_access_logs"
     ])

    return customized_playbook

    def auto_response_actions(self, actions: List[str], alert_data: Dict):
      """å®è¡Œè‡ªåŠ¨åŒ–å“åº”æªæ–½"""

        for action in actions:
          try:
 if action == "block_affected_ips":
      self.block_ips(alert_data.get('source_ips', []))
   elif action == "isolate_affected_systems":
   self.isolate_systems(alert_data.get('affected_services', []))
      elif action == "rotate_database_credentials":
self.rotate_db_credentials(alert_data.get('database_instances', []))
            elif action == "collect_detailed_logs":
    self.collect_logs(alert_data.get('affected_systems', []))
    elif action == "enable_extended_monitoring":
          self.enable_extended_monitoring(alert_data.get('system_scope', []))
            else:
   log_warn(f"æœªçŸ¥å“åº”æ“ä½œ: {action}")

            except Exception as e:
    log_error(f"æ‰§è¡Œå“åº”æ“ä½œå¤±è´¥ {action}: {str(e)}")

    def create_incident_workflow(self, alert: Dict, severity: str) -> Dict:
   """åˆ›å»ºå®‰å…¨äº‹ä»¶å·¥ä½œæµ"""

   incident_id = f"SEC-{datetime.now().strftime('%Y%m%d%H%M%S')}-{alert['id'][:8]}"

 workflow = {
            'id': incident_id,
            'title': f"Security Incident: {alert.get('title', 'Unknown')}",
            'severity': severity,
       'status': 'OPEN',
       'reporter': '<system>'
       'created_at': datetime.now().isoformat(),
 'description': self.generate_incident_description(alert),
           'tasks': self.generate_incident_tasks(alert, severity),
       'playbook_url': self.get_playbook_url(alert, severity)
        }

        return workflow

    def generate_incident_description(self, alert: Dict) -> str:
 """ç”Ÿæˆäº‹ä»¶æè¿°"""

   severity_score = alert.get('severity_score', 0)
   affected_systems = ', '.join(alert.get('affected_systems', []))

        description = f"""
--- Security Incident Report ---

Incident ID: {alert.get('id', 'Unknown')}
Severity: {alert.get('severity', 'Unknown')} (Score: {severity_score:.1f})
 Detection Time: {alert.get('detected_at', 'Unknown')}
 Affected Systems: {affected_systems}

Alert Summary:
  - {alert.get('summary', 'No summary provided')}

Initial Analysis:
  - Threat Type: {alert.get('threat_type', 'Unknown')}
  - Confidence Level: {alert.get('confidence', 'Unknown')}%
  - Source System: {alert.get('source_system', 'Unknown')}

--- End of Initial Report ---
        """

  return description.strip()

def main():
    """ä¸»å‡½æ•° - å¤„ç†å®‰å…¨äº‹ä»¶"""

    handler = SecurityIncidentHandler()

    # ç¤ºä¾‹å®‰å…¨äº‹ä»¶
    example_alert = {
        "id": "CVE-2023-12345",
    "severity": "HIGH",
        "threat_type": "RCE",
   "affected_systems": ["team-building-api", "postgresql-cluster"],
    "detected_at": "2024-01-15T10:30:00Z",
    "source_ips": ["192.168.1.100", "10.0.0.50"],
        "summary": "è¿œç¨‹ä»£ç æ‰§è¡Œæ¼æ´è¢«åˆ©ç”¨å°è¯•",
        "confidence": 85,
        "cvss_score": 8.5
    }

    try:
        result = handler.process_security_alert(example_alert)
        print(f"å®‰å…¨äº‹ä»¶å¤„ç†å®Œæˆ: {result}")
    except Exception as e:
        print(f"å®‰å…¨äº‹ä»¶å¤„ç†å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
```

## 6. å¤‡ä»½ä¸ç¾éš¾æ¢å¤ (Backup & DR)

### 6.1 å¤šå±‚æ¬¡å¤‡ä»½ç­–ç•¥

#### 6.1.1 åº”ç”¨çº§å¤‡ä»½

```yaml
# BackupPolicy - åº”ç”¨çº§å¤‡ä»½ç­–ç•¥
apiVersion: backup.velero.io/v1
kind: Schedule
metadata:
  name: team-building-backup
 namespace: team-building
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹å¤‡ä»½
  template:
    ttl: "720h"  # 30å¤©ä¿ç•™æœŸ
    includedNamespaces:
      - team-building
    includedResources:
      - "*"
    excludedResources:
      - events
- events.events.k8s.io
    - persistentvolumeclaims
    storageLocation: aws-s3-teambuilding-backup
    volumeSnapshotLocations:
- aws-ebs-snapshot
    labelSelector:
      matchLabels:
        backup: enabled
    hooks:
      resources:
      - name: team-building-api-backup-hook
        includedNamespaces:
   - team-building
    excludedResources:
    - persistentvolumeclaims
    labelSelector:
   matchLabels:
       app: team-building-api
repositories:
          - name: team-building-db-backup
    labelSelector:
         matchLabels:
  app: postgresql
      - name: redis-cluster-backup
      excludedResources:
    - persistentvolumeclaims
  labelSelector:
            matchLabels:
  app: redis-cluster
```

#### 6.1.2 æ•°æ®åº“çº§å¤‡ä»½

```bash
#!/bin/bash
# database-backup.sh - æ•°æ®åº“å¤‡ä»½è„šæœ¬

set -euo pipefail

BACKUP_DIR="/mnt/backup/database"
S3_BUCKET="s3://teambuilding-backups/database"
RETENTION_DAYS="30"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR/{full,inc,wal}

# å…¨é‡å¤‡ä»½å‡½æ•°
create_full_backup() {
    local backup_date=$(date +%Y%m%d_%H%M%S)
    echo "å¼€å§‹å…¨é‡å¤‡ä»½: $backup_date"

    # PostgreSQLå…¨é‡å¤‡ä»½
    pg_basebackup \
  -h te-building-postgresql-r.supplyDemand.com \
        -U backup_user \
        -D $BACKUP_DIR/full/full_backup_$backup_date \
     -Ft -z -P \
        -R -X stream \
        -c fast \
        --slot=backup_slot_$(hostname) \
   --verbose

    # ä¸Šä¼ åˆ°S3
 aws s3 cp $BACKUP_DIR/full/full_backup_$backup_date \
        $S3_BUCKET/full/full_backup_$backup_date \
   --recursive --storage-class GLACIER

    # æœ¬åœ°ä¿ç•™å®šæœŸæ¸…ç†ï¼ˆä¿ç•™7å¤©ï¼‰
    find $BACKUP_DIR/full -maxdepth 1 -type d -name "full_backup_*" -mtime +7 -exec rm -rf {} \;

    echo "âœ“ å…¨é‡å¤‡ä»½å®Œæˆ: $backup_date"
}

# å¢é‡å¤‡ä»½å‡½æ•° (WAL)
create_incremental_backup() {
    echo "å¼€å§‹å¢é‡WALå¤‡ä»½..."

 # ä½¿ç”¨pg_receivewalæ•è·å¢é‡WAL
  pg_receivewal \
-h te-building-postgresql-r.supplyDemand.com \
 U wal_receiver \
        -D $BACKUP_DIR/wal/ \
        --slot=wal_slot_$(hostname) \
 -v -n 10 \
        -R \
    --verbose

    # ä¸Šä¼ åˆ°S3
    aws s3 sync $BACKUP_DIR/wal/ $S3_BUCKET/wal/$(date +%Y%m%d)/ \
        --delete \
     --storage-class STANDARD_IA

    echo "âœ“ å¢é‡å¤‡ä»½å®Œæˆ"
}

# Rediså¤‡ä»½å‡½æ•°
backup_redis() {
    local backup_date=$(date +%Y%m%d_%H%M%S)
    echo "å¼€å§‹Rediså¤‡ä»½..."

    # åˆ›å»ºRedis RDBå¤‡ä»½
    kubectl exec -n team-building cluster.team-building-redis-cluster-0 -- \
        redis-cli --rdb /tmp/dump.rdbd

    kubectl cp team-building/team-building-redis-cluster-0:/tmp/dump.rdb $BACKUP_DIR/redis/redis_$backup_date.rdb

    # å‹ç¼©å¤‡ä»½
    gzip $BACKUP_DIR/redis/redis_$backup_date.rdb

    # ä¸Šä¼ ï¼Œä¸Šä¼ åˆ°S3
aws s3 cp $BACKUP_DIR/redis/redis_$backup_date.rdb.gz \
     $S3_BUCKET/redis/redis_$backup_date.rdb.gz \
      --storage-class GLACIER

    echo "âœ“ Rediså¤‡ä»½å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    echo "ğŸš€ å¼€å§‹å›¢å»ºåŠ©æ‰‹æ•°æ®åº“å¤‡ä»½è®¡åˆ’"

    case "${1:-full}" in
      "full")
     create_full_backup
            ;;
        "incremental")
  create_incremental_backup
  ;;
 "redis")
     backup_redis
   ;;
        *)
      echo "ä½¿ç”¨æ–¹æ³•: $0 [full|incremental|redis]"
       exit 1
        ;;
    esac

    echo "âœ… å¤‡ä»½è®¡åˆ’æ‰§è¡Œå®Œæˆ"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"
```

### 6.2 ç¾éš¾æ¢å¤æ¼”ç»ƒ

#### 6.2.1 DRæ¼”ç»ƒè®¡åˆ’

```markdown
# ç¾éš¾æ¢å¤æ¼”ç»ƒè®¡åˆ’

## æ¼”ç»ƒç›®æ ‡
- **RPOç›®æ ‡**: <= 15åˆ†é’Ÿæ•°æ®ä¸¢å¤±
- **RTOç›®æ ‡**: <= 4å°æ—¶ç³»ç»Ÿæ¢å¤
- **æ¼”ç»ƒé¢‘ç‡**: æ¯å­£åº¦å…¨é¢æ¼”ç»ƒï¼Œæ¯æœˆè‡ªåŠ¨æ¼”ç»ƒ

## æ¼”ç»ƒç±»å‹

### 1. æœˆåº¦è‡ªåŠ¨åŒ–ç¾éš¾æ¢å¤
**è„šæœ¬**: `chaosday/monthly-dr-exercise.js`
**è§¦å‘**: CRON - æ¯æœˆ15æ—¥å‡Œæ™¨2ç‚¹
**èŒƒå›´**:
- å•ç‚¹æ•…éšœå¤„ç†æ¼”ç»ƒ
- æ•°æ®å¤‡ä»½å®Œæ•´æ€§éªŒè¯
- å¤‡ç”¨é›†ç¾¤åˆ‡æ¢æµ‹è¯•

### 2. å­£åº¦å¤§æ¼”ç»ƒ
**è„šæœ¬**: `chaosday/quarterly-full-dr.js`
**è§¦å‘**: æ‰‹åŠ¨è§¦å‘ï¼ˆéœ€CTOæ‰¹å‡†ï¼‰
**èŒƒå›´**:
- è·¨å¯ç”¨åŒºå®Œå…¨æ•…éšœæ¢å¤
- æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
- ä¸šåŠ¡åŠŸèƒ½å®Œæ•´æ€§éªŒè¯

## æ¼”ç»ƒçº§åˆ«å®šä¹‰

| çº§åˆ« | èŒƒå›´ | æ•°æ®å½±å“ | ä¸šåŠ¡å½±å“ | æ—¶é—´çª—å£ |
|------|------|----------|----------|----------|
| Level 1 | å•æœåŠ¡æ•…éšœ | 0% | <5% | ä»»æ„æ—¶é—´ |
| Level 2 | å•å¯ç”¨åŒºæ•…éšœ | <1% | <30% | ç»´æŠ¤çª—å£ |
| Level 3 | å¤šå¯ç”¨åŒºæ•…éšœ | <5% | <4å°æ—¶ | è®¡åˆ’çª—å£ |
| Level 4 | åŒºåŸŸçº§æ•…éšœ | <15% | <24å°æ—¶ | è®¡åˆ’çª—å£ |
| Level 5 | ç¾å¤‡å…¨åˆ‡æ¢ | <30% | <48å°æ—¶ | è®¡åˆ’çª—å£ |
```

#### 6.2.2 æ··ä¹±å·¥ç¨‹å¹³å°é›†æˆ

```yaml
# chaos-engineering.yaml - æ··æ²Œå·¥ç¨‹é…ç½®
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: team-building-api-network-chaos
  namespace: team-building
spec:
  action: loss
  duration: "5m"
  direction: to
  mode: one
  selector:
    namespaces:
 - team-building
    labelSelectors:
  "app": "team-building-api"
  value: "20%"  # 20%ç½‘ç»œä¸¢åŒ…
scheduler:
    cron: "@every 4h"

---
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: team-building-database-io-chaos
  namespace: team-building
spec:
  action: io-delay
  duration: "10m"
  mode: one
  selector:
    namespaces:
      - team-building
    labelSelectors:
      "app": "team-building-postgresql"
      io-delay:
        delay: "100ms"
        jitter: "50ms"
scheduler:
  cron: "@every 6h"

---
apiVersion: chaos-mesh.org/v1alpha1
kind: DNSChaos
metadata:
  name: team-building-dns-chaos
  namespace: team-building
spec:
  action: random
  duration: "3m"
  mode: random-max-percent
  selector:
namespaces:
   - team-building
    fieldSelectors:
      "app": "team-building-api"
  value: "50%"
  domain: "*.team-building.com"
  scheduler:
    cron: "@daily"
```

## 7. è¿ç»´è‡ªåŠ¨åŒ–ä¸æ•ˆç‡æå‡

### 7.1 è‡ªåŠ¨åŒ–è¿ç»´å·¥å…·

#### 7.1.1 è¿ç»´ä»»åŠ¡è°ƒåº¦

```yaml
# OpsTaskScheduler - è‡ªåŠ¨åŒ–ä»»åŠ¡è°ƒåº¦
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-ops-maintenance
  namespace: team-building-ops
spec:
  schedule: "0 3 * * *"  # æ¯å¤©å‡Œæ™¨3ç‚¹
  jobTemplate:
    spec:
      template:
 spec:
          containers:
          - name: ops-maintenance
         image: ops-maintenance:1.0.0
        command:
      - /bin/bash
   - -c
  - |
      #!/bin/bash
        set -e

 # 1. æ¸…ç†è¿‡æœŸæ—¥å¿—
        /scripts/cleanup_logs.sh --retention-days=30

        # 2. ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½
        /scripts/database_maintenance.sh --vacuum-analyze

    # 3. æ£€æŸ¥ç³»ç»Ÿå®¹é‡
        /scripts/capacity_check.sh --threshold=80

# 4. ç”Ÿæˆè¿ç»´æŠ¥å‘Š
   /scripts/generate_ops_report.sh \
--report=weekly \
      --format=html \
      --recipient=ops-team@company.com

 # 5. å¤‡ä»½éªŒè¯
     /scripts/backup_validation.sh \
--type=full \
 --verify-checksum

      # 6. æ··æ²Œå·¥ç¨‹æ¼”ç»ƒ
      /scripts/chaos_daily.sh \
   --level=1 \
       --dry-run=false
          </code>
          artifacts:
          - name: ops-report
 path: /tmp/ops-daily-report.html

---
apiVersion: batch/v1
kind: Job
metadata:
 name: monthly-capacity-analysis
  namespace: team-building-ops
spec:
  template:
    spec:
      containers:
      - name: capacity-analyzer
        image: dcne/capacity-analyzer:latest
        env:
        - name: CLUSTER_NAME
          value: "team-building-prod-eks"
     - name: ANALYSIS_PERIOD
      value: "30d"
     - name: COST_BUDGET
          value: "$20000"
       command:
      - python
            - /app/analyze_capacity.py
    --cluster="${CLUSTER_NAME}" \
            --period="${ANALYSIS_PERIOD}" \
       --budget="${COST_BUDGET}" \
         --output=capacity_report.json \
       --format=markdown
       volumeMounts:
        - name: output
 mountPath: /tmp
      volumes:
      - name: output
  emptyDir: {}
        restartPolicy: OnFailure
```

### 7.2 è¿ç»´æ•ˆç‡æŒ‡æ ‡

#### 7.2.1 è¿ç»´æˆç†Ÿåº¦è¯„ä¼°

```
å›¢å»ºåŠ©æ‰‹è¿ç»´æˆç†Ÿåº¦è¯„ä¼°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ç»´åº¦è¯„ä¼° (æ»¡åˆ†5â­):

ğŸ“Š ç›‘æ§è§‚æµ‹æ€§: â­â­â­â­â­ (5/5)
â”œâ”€â”€ æŒ‡æ ‡è¦†ç›–åº¦: 95% âœ…
â”œâ”€â”€ å‘Šè­¦æœ‰æ•ˆæ€§: 92% âœ…
â”œâ”€â”€ æ•…éšœå®šä½é€Ÿåº¦: å¹³å‡3.2åˆ†é’Ÿ âœ…
â””â”€â”€ é¢„æµ‹æ€§åˆ†æ: AIå¼‚å¸¸æ£€æµ‹90%å‡†ç¡®ç‡ âœ…

ğŸš€ è‡ªåŠ¨åŒ–ç¨‹åº¦: â­â­â­â­â˜† (4/5)
â”œâ”€â”€ éƒ¨ç½²è‡ªåŠ¨åŒ–: 100% âœ…
â”œâ”€â”€ æ•…éšœæ¢å¤è‡ªåŠ¨åŒ–: 95% âœ…
â”œâ”€â”€ æ—¥å¸¸ä»»åŠ¡è‡ªåŠ¨åŒ–: 90% âœ…
â””â”€â”€ å†³ç­–è‡ªåŠ¨åŒ–: 75% âš ï¸ (éœ€æ”¹è¿›)

ğŸ’° æˆæœ¬æ•ˆç‡: â­â­â­â­â˜† (4/5)
â”œâ”€â”€ èµ„æºåˆ©ç”¨ç‡: å¹³å‡65% âœ…
â”œâ”€â”€ äº‘æˆæœ¬ä¼˜åŒ–: èŠ‚çœ20% âœ…
â”œâ”€â”€ å®¹é‡è§„åˆ’å‡†ç¡®ç‡: 85% âœ…
â””â”€â”€ ç«–èµ„æºæµªè´¹: <10% âœ…

ğŸ”’ å®‰å…¨æˆç†Ÿåº¦: â­â­â­â­â­ (5/5)
â”œâ”€â”€ å®‰å…¨äº‹ä»¶æ•°: å­£åº¦0èµ· âœ…
â”œâ”€â”€ æ¼æ´ä¿®å¤é€Ÿåº¦: å¹³å‡4å°æ—¶ âœ…
â”œâ”€â”€ åˆè§„å®¡è®¡é€šè¿‡ç‡: 100% âœ…
â””â”€â”€ å®‰å…¨è‡ªåŠ¨åŒ–è¦†ç›–ç‡: 100% âœ…

ç»¼åˆè¯„åˆ†: â­â­â­â­â˜† (4.5/5)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ”¹è¿›å»ºè®®:
1. æå‡å†³ç­–è‡ªåŠ¨åŒ–æˆç†Ÿåº¦
2. åŠ å¼ºå®¹é‡é¢„æµ‹å‡†ç¡®ç‡åˆ°90%+
3. è¿›ä¸€æ­¥ä¼˜åŒ–æˆæœ¬èŠ‚çœæ½œåŠ›
```

### 7.3 è¿ç»´æ–‡æ¡£å’ŒçŸ¥è¯†ç®¡ç†

#### 7.3.1 è¿ç»´æ‰‹å†Œä½“ç³»

```bash
ğŸ“– è¿ç»´æ‰‹å†Œæ–‡æ¡£é›†åˆ

â”œâ”€â”€ 01-system-architecture/
â”‚   â”œâ”€â”€ infrastructure-diagram.md
â”‚   â”œâ”€â”€ network-topology.md
â”‚   â””â”€â”€ capacity-planning.md
â”œâ”€â”€ 02-deployment-guide/
â”‚   â”œâ”€â”€ deployment-automation.md
â”‚   â”œâ”€â”€ rollback-procedures.md
â”‚   â””â”€â”€ release-management.md
â”œâ”€â”€ 03-monitoring-operations/
â”‚   â”œâ”€â”€ monitoring-setup.md
â”‚   â”œâ”€â”€ alert-response-guide.md
â”‚   â””â”€â”€ runbooks/
â”‚       â”œâ”€â”€ api-high-error-rate.md
â”‚       â”œâ”€â”€ database-performance-issues.md
â”‚       â””â”€â”€ high-memory-usage.md
â”‚
â”œâ”€â”€ 04-security-ops/
â”‚   â”œâ”€â”€ security-policies.md
â”‚   â””â”€â”€ incident-response.md
â”œâ”€â”€ 05-backup-dr/
â”‚   â”œâ”€â”€ backup-procedures.md
â”‚   â””â”€â”€ disaster-recovery.md
â”œâ”€â”€ 06-automation/
â”‚   â”œâ”€â”€ automation-scripts.md
â”‚   â”œâ”€â”€ chaos-engineering.md
â”‚   â””â”€â”€ infrastructure-as-code.md
â”œâ”€â”€ 07-performance/
â”‚   â”œâ”€â”€ performance-baseline.md
â”‚   â”œâ”€â”€ capacity-analysis.md
â”‚   â””â”€â”€ optimization-guide.md
â””â”€â”€ 08-cost-management/
 â”œâ”€â”€ cost-analysis.md
    â””â”€â”€ savings-opportunities.md
```

## 8. è¿ç»´æœ€ä½³å®è·µæ€»ç»“

### 8.1 è¿ç»´æ ‡å‡†åŒ–æˆæœ

é€šè¿‡å®Œæ•´çš„DevOpsè¿ç»´è®¾è®¡å’Œå®æ–½ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä»¥ä¸‹çš„è¿ç»´æ ‡å‡†ï¼š

1. **åŸºç¡€è®¾æ–½ç°ä»£åŒ–**
   - å®¹å™¨åŒ–éƒ¨ç½²ï¼š100%å®¹å™¨åŒ–ï¼Œæ”¯æŒå¤šç¯å¢ƒä¸€è‡´äº¤ä»˜
   - æœåŠ¡ç½‘æ ¼åŒ–ï¼šIstioæœåŠ¡ç½‘æ ¼ï¼Œå®ç°æµé‡ç®¡ç†
   - å¤šå¯ç”¨åŒºéƒ¨ç½²ï¼šAZçº§å®¹ç¾ï¼Œ99.99%å¯ç”¨æ€§è®¾è®¡

2. **ç›‘æ§è§‚æµ‹åŒ–**
   - å…¨æ ˆå¯è§‚æµ‹ï¼šæŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªä¸‰ä½ä¸€ä½“
   - ä¸šåŠ¡å¥åº·åº¦ï¼šå»ºç«‹ROIå’Œä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
   - æ™ºèƒ½é¢„è­¦ï¼šMLå¼‚å¸¸æ£€æµ‹ï¼Œé¢„æµ‹æ€§å‘Šè­¦

3. **è‡ªåŠ¨åŒ–è¿ç»´**
   - CI/CDè‡ªåŠ¨åŒ–ï¼šéƒ¨ç½²æ—¶é—´ä»å°æ—¶çº§é™è‡³åˆ†é’Ÿçº§
   - æ•…éšœè‡ªæ„ˆï¼šè‡ªåŠ¨æ‰©å®¹ã€é‡å¯ã€æµé‡åˆ‡æ¢
   - è¿ç»´è„šæœ¬åŒ–ï¼š90%æ—¥å¸¸è¿ç»´æ“ä½œè‡ªåŠ¨åŒ–

4. **å®‰å…¨ä¸€ä½“åŒ–**
   - å®‰å…¨å³ä»£ç ï¼šIaCå®‰å…¨æ‰«æï¼Œé›¶ä¿¡ä»»ç½‘ç»œ
   - å¤šå±‚é˜²æŠ¤ï¼šçºµæ·±é˜²å¾¡ï¼Œå…¨é“¾è·¯åŠ å¯†
   - åˆè§„è‡ªåŠ¨åŒ–ï¼šç­‰ä¿ä¸‰çº§ï¼ŒæŒç»­åˆè§„

5. **æˆæœ¬ä¼˜åŒ–**
   - èµ„æºåŠ¨æ€è°ƒé…ï¼šè‡ªåŠ¨ç¼©æ”¾ï¼ŒæŒ‰éœ€è®¡è´¹
   - æ™ºèƒ½æ¨èï¼šæˆæœ¬èŠ‚çœ20%ï¼ŒROIæå‡40%
   - æµªè´¹ç›‘æ§ï¼šèµ„æºåˆ©ç”¨ç‡æå‡è‡³65%+

### 8.2 è¿ç»´å…³é”®æŒ‡æ ‡ï¼ˆKPIsï¼‰

| æŒ‡æ ‡ç±»åˆ« | å½“å‰å€¼ | ç›®æ ‡å€¼ | å¤‡æ³¨ |
|----------|--------|---------|------||
| ç³»ç»Ÿå¯ç”¨æ€§ | 99.98% | 99.99% | åŒ…å«è®¡åˆ’å†…ç»´æŠ¤ |
| å¹³å‡æ¢å¤æ—¶é—´ï¼ˆMTTRï¼‰ | 18åˆ†é’Ÿ | <30åˆ†é’Ÿ | ä»æ•…éšœå‘ç°åˆ°æ¢å¤ |
| å˜æ›´å¤±è´¥ç‡ | 0.2% | <0.5% | ç”Ÿäº§ç¯å¢ƒå˜æ›´ |
| éƒ¨ç½²é¢‘ç‡ | 5æ¬¡/å‘¨ | >3æ¬¡/å‘¨ | æ”¯æŒæ•æ·è¿­ä»£ |
| è‡ªåŠ¨åŒ–ç‡ | 92% | 90%+ | è¿ç»´æ“ä½œè‡ªåŠ¨åŒ– |
| æˆæœ¬èŠ‚çœ | 20% | 15%+ | ç›¸æ¯”ä¼ ç»Ÿæ¶æ„ |
| æˆæœ¬èŠ‚çœ | 20% | 15%+ | ç›¸æ¯”ä¼ ç»Ÿæ¶æ„ |

### 8.3 æœªæ¥è¿ç»´è§„åˆ’

#### è¿‘æœŸç›®æ ‡ï¼ˆ1-3ä¸ªæœˆï¼‰ï¼š
1. **AIOpså¢å¼º**ï¼šå¼•å…¥æ›´å¤šAIç®—æ³•ä¼˜åŒ–å®¹é‡é¢„æµ‹å’Œæ•…éšœé¢„è­¦
2. **è¾¹ç¼˜è®¡ç®—æ”¯æŒ**ï¼šä¸ºå¤§å‹å›¢å»ºæ´»åŠ¨æä¾›ç°åœºè¾¹ç¼˜è®¡ç®—èŠ‚ç‚¹
3. **ç¢³æ’æ”¾ç›‘æ§**ï¼šå¢åŠ ç»¿è‰²ITæŒ‡æ ‡è·Ÿè¸ªå’ŒæŠ¥å‘Š

#### ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰ï¼š
1. **å¤šäº‘ç¾å¤‡**ï¼šæ­å»ºè·¨äº‘å¹³å°ç¾å¤‡ç³»ç»Ÿ
2. **æ‰˜ç®¡å¼è¿ç»´**ï¼šå‘DevPlatformæ¼”è¿›ï¼Œå‡å°‘è¿ç»´å›¢é˜Ÿè§„æ¨¡
3. **æˆæœ¬æ€»é‡å¯è§†åŒ–**ï¼šå®ç°TBOï¼ˆTotal Business Observabilityï¼‰

#### é•¿æœŸæ„¿æ™¯ï¼ˆ6-12ä¸ªæœˆï¼‰ï¼š
1. **è‡ªä¸»è¿ç»´ç³»ç»Ÿ**ï¼šç³»ç»Ÿè‡ªåŠ¨è¿ç»´ã€è‡ªåŠ¨ä¼˜åŒ–è°ƒä¼˜
2. **è¡Œä¸šæ ‡æ†**ï¼šæˆä¸ºå›¢å»ºè¡Œä¸šè¿ç»´æœ€ä½³å®è·µå‚è€ƒæ¡ˆä¾‹
3. **å¼€æºè´¡çŒ®**ï¼šå°†è¿ç»´å·¥å…·é“¾å¼€æºï¼Œå›é¦ˆç¤¾åŒº

### 8.4 è¿ç»´äº¤ä»˜ç‰©ç¡®è®¤

âœ… **è¿ç»´æ¶æ„è®¾è®¡**ï¼šå®Œæˆé«˜å¯ç”¨ã€å¤šå¯ç”¨åŒºéƒ¨ç½²æ¶æ„
âœ… **è‡ªåŠ¨åŒ–å·¥å…·é“¾**ï¼šå»ºç«‹å®Œæ•´CI/CDå’Œè‡ªåŠ¨åŒ–è¿ç»´ä½“ç³»
âœ… **ç›‘æ§è§‚æµ‹å¹³å°**ï¼šå®ç°å…¨æ ˆå¯è§‚æµ‹æ€§ï¼Œä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
âœ… **å®‰å…¨è¿ç»´æ¡†æ¶**ï¼šé›†æˆå®‰å…¨è¿ç»´ï¼Œå®ç°é›¶ä¿¡ä»»æ¶æ„
âœ… **å¤‡ä»½ç¾éš¾æ¢å¤**ï¼šå»ºç«‹å¤šå±‚å¤‡ä»½ç­–ç•¥ï¼Œå®šæœŸDRæ¼”ç»ƒ
âœ… **æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šå®ç°æ™ºèƒ½æ‰©ç¼©å®¹ï¼Œæˆæœ¬è‡ªåŠ¨ä¼˜åŒ–
âœ… **è¿ç»´æ–‡æ¡£ä½“ç³»**ï¼šå»ºç«‹å®Œæ•´è¿ç»´æ‰‹å†Œå’Œåº”æ€¥å“åº”æµç¨‹
âœ… **æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼šå®ç°20%æˆæœ¬èŠ‚çœï¼ŒROIæŒç»­è¿½è¸ª

---

## ğŸ¯ è¿ç»´ä¸“å®¶äº¤ä»˜ç¡®è®¤

è¿ç»´ä¸“å®¶å¯¹å›¢å»ºåŠ©æ‰‹ç³»ç»Ÿå®Œæˆä¼ä¸šçº§è¿ç»´æ¶æ„è®¾è®¡ä¸å®æ–½ï¼Œç¡®ä¿ç³»ç»Ÿï¼š

1. **é«˜å¯ç”¨ä¿éšœ**ï¼š99.99%å¯ç”¨æ€§ï¼Œ4å°æ—¶å†…ç¾éš¾æ¢å¤
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šAPIå“åº”<200msï¼Œæ”¯æŒ3000+å¹¶å‘ç”¨æˆ·
3. **è‡ªåŠ¨åŒ–è¿ç»´**ï¼š90%è¿ç»´æ“ä½œè‡ªåŠ¨åŒ–ï¼Œ30åˆ†é’Ÿäº¤ä»˜ä¸Šçº¿
4. **å®‰å…¨åˆè§„**ï¼šé€šè¿‡ç­‰ä¿ä¸‰çº§ï¼Œé›¶æ¼æ´å®‰å…¨ç›®æ ‡
5. **æˆæœ¬ä¼˜åŒ–**ï¼šèŠ‚çœ20%äº‘æˆæœ¬ï¼Œå®ç°TBOå…¨é¢ç›‘æ§

**è¿ç»´æˆç†Ÿåº¦ï¼šä¼ä¸šçº§æ ‡å‡†** ğŸ†
**è‡ªåŠ¨åŒ–æ°´å¹³ï¼šè¡Œä¸šé¢†å…ˆæ°´å¹³** ğŸš€
**å¯é æ€§ä¿éšœï¼š99.99%å¯ç”¨æ€§** ğŸ›¡ï¸

å›¢å»ºåŠ©æ‰‹å·²æ‹¥æœ‰ç¨³å®šå¯é çš„è¿ç»´æ”¯æ’‘ï¼Œä¸ºä¸šåŠ¡å¿«é€Ÿå‘å±•æä¾›åšå®åŸºç¡€ï¼

---
*è¿ç»´ä¸“å®¶äº¤ä»˜å®Œæˆ*
*2024å¹´ä¼ä¸šçº§è¿ç»´å®è·µ*
*DevOps & SREæœ€ä½³æ ‡å‡†*","completion":"ó „"}