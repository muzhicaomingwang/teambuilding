# æ•°æ®åº“ç®¡ç†å‘˜äº§å‡º - å›¢å»ºåŠ©æ‰‹ (Database Administrator Output - Team Building Assistant)

**æ•°æ®åº“ç®¡ç†å‘˜ï¼ˆDBAï¼‰è¾“å‡ºæ–‡æ¡£**

## 1. æ•°æ®åº“æ¶æ„è®¾è®¡æ¦‚è¿° (Database Architecture Overview)

### 1.1 æ•°æ®å­˜å‚¨ç­–ç•¥

å›¢å»ºåŠ©æ‰‹ç³»ç»Ÿé‡‡ç”¨å¤šæ¨¡æ€æ•°æ®å­˜å‚¨æ¶æ„ï¼Œé’ˆå¯¹ä¸åŒæ•°æ®ç‰¹æ€§é€‰æ‹©æœ€é€‚åˆçš„æ•°æ®åº“æŠ€æœ¯ï¼š

```mermaid
graph TD
    subgraph "ä¸šåŠ¡æ•°æ®å±‚"
        A[PostgreSQL 15] --> B[æ´»åŠ¨æ•°æ®]
 A --> C[å›¢é˜Ÿæ•°æ®]
        A --> D[ç”¨æˆ·æ•°æ®]
        A --> E[é¢„ç®—æ•°æ®]
    end

    subgraph "ç¼“å­˜åŠ é€Ÿå±‚"
        F[Redis Cluster] --> G[ä¼šè¯ç¼“å­˜]
     F --> H[çƒ­ç‚¹æ•°æ®]
    F --> I[åˆ†å¸ƒå¼é”]
        F --> J[å®æ—¶åä½œ]
    end

    subgraph "åˆ†æå­˜å‚¨å±‚"
        K[Elasticsearch 8] --> L[å…¨æ–‡æœç´¢]
  K --> M[æ—¥å¿—åˆ†æ]
        K --> N[ç”¨æˆ·è¡Œä¸ºæ•°æ®]
    end

    subgraph "æ–‡ä»¶å­˜å‚¨å±‚"
    O[MinIO] --> P[å›¾ç‰‡è§†é¢‘]
        O --> Q[æ–‡æ¡£æ–‡ä»¶]
        O --> R[å¯¼å‡ºæŠ¥å‘Š]
    end
```

### 1.2 æ•°æ®åº“é€‰å‹ç†ç”±

| æ•°æ®åº“ç±»å‹ | é€‰æ‹©ç†ç”± | é€‚ç”¨åœºæ™¯ |
|------------|----------|----------|
| **PostgreSQL 15** | - ACIDäº‹åŠ¡æ”¯æŒå®Œå–„<br>- JSONBåŸç”Ÿæ”¯æŒ<br>- çª—å£å‡½æ•°å’ŒCTEå¼ºå¤§<br>- æ‰©å±•ä¸°å¯Œ(PostGIS,pg_partman) | æ ¸å¿ƒä¸šåŠ¡æ•°æ®å­˜å‚¨ |
| **Redis Cluster** | - å†…å­˜çº§æ€§èƒ½<br>- å‘å¸ƒè®¢é˜…åŠŸèƒ½<br>- åŸå­æ“ä½œæ”¯æŒ<br>- æ•°æ®è¿‡æœŸæœºåˆ¶ | ä¼šè¯ç®¡ç†ã€ç¼“å­˜ã€åˆ†å¸ƒå¼é” |
| **Elasticsearch 8** | - å…¨æ–‡æœç´¢èƒ½åŠ›<br>- è¿‘å®æ—¶æœç´¢<br>- èšåˆåˆ†æåŠŸèƒ½<br>- æ°´å¹³æ‰©å±•èƒ½åŠ› | æ´»åŠ¨æœç´¢ã€æ—¥å¿—åˆ†æã€ç”¨æˆ·è¡Œä¸º |
| **MinIO** | - S3 APIå…¼å®¹<br>- é«˜æ€§èƒ½å¯¹è±¡å­˜å‚¨<br>- åˆ†å¸ƒå¼æ¶æ„<br>- æ•°æ®æŒä¹…æ€§ | æ–‡ä»¶å­˜å‚¨ã€åª’ä½“èµ„æºã€å¤‡ä»½æ•°æ® |

## 2. PostgreSQLé›†ç¾¤è®¾è®¡ (PostgreSQL Cluster Design)

### 2.1 é«˜å¯ç”¨æ¶æ„è®¾è®¡

#### 2.1.1 Patronié›†ç¾¤æ–¹æ¡ˆ

```yaml
# Patronié…ç½®æ–‡ä»¶
bootstrap:
  dcs:
  # DCS (Distributed Configuration Store) é…ç½®
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        max_connections: 200
shared_buffers: 256MB
        effective_cache_size: 2GB
      work_mem: 4MB
        maintenance_work_mem: 64MB
      wal_buffers: 16MB
        checkpoint_completion_target: 0.9
        wal_compression: on
    random_page_cost: 1.1
        effective_io_concurrency: 200
default_statistics_target: 100
        # å¤åˆ¶ç›¸å…³å‚æ•°
        wal_level: replica
        hot_standby: on
        max_wal_senders: 10
     max_replication_slots: 10
 hot_standby_feedback: on
    # ä¸€è‡´æ€§ä¿éšœ
        synchronous_commit: on
        synchronous_standby_names: "*"

  # åˆå§‹åŒ–æ•°æ®åº“
  initdb:
  - encoding: UTF8
    - locale: en_US.UTF-8
    - data-checksums

  pg_hba:
  - hostssl all all 0.0.0.0/0 md5
    - hostssl all all ::0/0 md5
  - host all all 127.0.0.1/32 trust

postgresql:
  authentication:
replication:
      username: replicator
  password: "{{env `PATRONI_REPLICATION_PASSWORD`}}"
        superuser:
          postgres:
  username: postgres
     password: "{{env `POSTGRES_PASSWORD`}}"
    parameters:
      unix_socket_directories: '.'
      port: 5432
    recovery_conf:
      restore_command: "/usr/bin/envdir /etc/wal-g.d/env /usr/local/bin/wal-g wal-fetch %f %p"
  # å¤‡ç”¨æœåŠ¡å™¨é…ç½®
 standby_cluster:
      host: master.te-building-postgresql-r.supplyDemand.com
        port: 5432
primary_slot_name: standby_slot

# é›†ç¾¤æˆå‘˜é…ç½®100
scope: team-building-postgres-cluster
name: postgresql-{{ .Values.replicaIndex }}

restapi:
  listen: 0.0.0.0:8008
  connect_address: postgresql-{{ .Values.replicaIndex }}.te-building-postgresql-r.supplyDemand.com:8008

etcd:
  hosts:
  - etcd-0.team-building-etcd:2379
    - etcd-1.team-building-etcd:2379
    - etcd-2.team-building-etcd:2379

# WAL-Gå¤‡ä»½é…ç½® (é«˜çº§ç‰¹æ€§)
it 'backup'
    create_replica_method:
 - wal_g
    wal_g:
      command: "/usr/local/scripts/wal_g_backup.sh"
      no_master: 1
      no_params: true
      keep_data: false
      running_timeout: 3600
      recovery_conf:
      restore_command: "/usr/bin/envdir /etc/wal-g.d/env /usr/local/bin/wal-g wal-fetch %f %p"
```

#### 2.1.2 pgBouncerè¿æ¥æ± é…ç½®

```ini
# pgBounceré…ç½® - è¿æ¥æ± ç®¡ç†
[databases]
team_building_primary = host=postgresql-primary port=5432 dbname=team_building
 team_building_standby = host=postgresql-standby port=5432 dbname=team_building
team_building_pool = host=postgresql-pool port=5432 dbname=team_building

[pgbouncer]
# åŸºæœ¬å‚æ•°
listen_addr = 0.0.0.0
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt

# è¿æ¥æ± å‚æ•°
pool_mode = transaction
max_client_conn = 300
default_pool_size = 50
min_pool_size = 15
reserve_pool_size = 5
reserve_pool_timeout = 3

# æŸ¥è¯¢æ—¥å¿—å’Œç»Ÿè®¡
stats_users = stats, monitor
log_connections = 1
log_disconnections = 1
log_pooler_stats = 1
stats_period = 60

# æŸ¥è¯¢é™åˆ¶
max_prepared_statements = 100
ignore_startup_parameters = extra_float_digits

# è¶…æ—¶è®¾ç½®
server_check_query = select 1
server_check_delay = 30
server_lifetime = 3600
server_idle_timeout = 600

# é«˜çº§ç‰¹æ€§
application_name_add_host = 1
max_user_connections = 30
conffile = /etc/pgbouncer/pgbouncer.ini
```

### 2.2 åˆ†ç‰‡ç­–ç•¥è®¾è®¡

#### 2.2.1 ä¸šåŠ¡æ•°æ®åˆ†ç‰‡ç­–ç•¥

åŸºäºä¸šåŠ¡ç‰¹æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹åˆ†ç‰‡ç­–ç•¥ï¼š

```sql
-- åˆ†ç‰‡ç­–ç•¥å®šä¹‰
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_partman";

-- æŒ‰å›¢é˜ŸIDåˆ†ç‰‡çš„æ´»åŠ¨è¡¨åˆ†åŒº
CREATE TABLE partitioned_activities (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    team_id UUID NOT NULL,
 title VARCHAR(200) NOT NULL,
  status activity_status NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
) PARTITION BY HASH (team_id);

-- åˆ›å»º16ä¸ªåˆ†åŒºï¼ˆæŒ‰å›¢é˜ŸIDå“ˆå¸Œï¼‰
CREATE TABLE partitioned_activities_p00 PARTITION OF partitioned_activities
FOR VALUES WITH (MODULUS 16, REMAINDER 0);
CREATE TABLE partitioned_activities_p01 PARTITION OF partitioned_activities
    FOR VALUES WITH (MODULUS 16, REMAINDER 1);
-- ... ç»§ç»­åˆ›å»ºp02åˆ°p15
```

#### 2.2.2 æ—¶é—´åˆ†åŒºè¡¨è®¾è®¡

å¯¹äºæ—¶é—´ç›¸å…³çš„æ•°æ®ï¼Œé‡‡ç”¨åˆ†åŒºä¼˜åŒ–ï¼š

```sql
-- æŒ‰æœˆä»½åˆ†åŒºçš„åé¦ˆæ•°æ®è¡¨
CREATE TABLE partitioned_feedback (
    id BIGSERIAL PRIMARY KEY,
    activity_id UUID NOT NULL,
    user_id UUID NOT NULL,
  rating INTEGER CHECK (rating BETWEEN 1 AND 5),
    comment TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- è‡ªåŠ¨åˆ†åŒºç®¡ç†ï¼ˆä½¿ç”¨pg_partmanï¼‰
SELECT create_parent('public.partitioned_feedback', 'created_at', 'partman', 'monthly', p_premake=>3);

-- æŸ¥çœ‹åˆ†åŒºä¿¡æ¯
SELECT partman.show_partition_name('public.partitioned_feedback');
SELECT partman.show_partition_info('public.partitioned_feedback');
```

### 2.3 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 2.3.1 æ•°æ®åº“æ€§èƒ½å‚æ•°è°ƒä¼˜

```sql
-- PostgreSQLæ€§èƒ½å‚æ•°Configuration
ALTER SYSTEM SET max_connections = 200;
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '1GB';
ALTER SYSTEM SET work_mem = '4MB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET default_statistics_target = 100;
ALTER SYSTEM SET random_page_cost = 1.1;
ALTER SYSTEM SET effective_io_concurrency = 200;
-- å¹¶è¡ŒæŸ¥è¯¢
ALTER SYSTEM SET max_parallel_workers_per_gather = 4;
ALTER SYSTEM SET max_parallel_workers = 8;
ALTER SYSTEM SET max_parallel_maintenance_workers = 4;
-- è‡ªåŠ¨vacuumè°ƒä¼˜
ALTER SYSTEM SET autovacuum_vacuum_scale_factor = 0.2;
ALTER SYSTEM SET autovacuum_analyze_scale_factor = 0.1;
ALTER SYSTEM SET autovacuum_vacuum_threshold = 50;
ALTER SYSTEM SET autovacuum_analyze_threshold = 50;
-- è¿æ¥æ± ä¼˜åŒ–
ALTER SYSTEM SET max_prepared_transactions = 100;
ALTER SYSTEM SET idle_in_transaction_session_timeout = '30min';
SELECT pg_reload_conf();
```

#### 2.3.2 æŸ¥è¯¢ä¼˜åŒ–å’Œç´¢å¼•ç­–ç•¥

```sql
-- é‡ç‚¹è¡¨ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

-- 1. æ´»åŠ¨è¡¨æ ¸å¿ƒç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_activities_team_date_status
ON activities (team_id, created_at DESC, status)
WHERE status IN ('APPROVED', 'IN_PROGRESS', 'COMPLETED');

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_activities_search
ON activities USING gin(to_tsvector('english', title || ' ' || coalesce(description, '')));

-- 2. å‚ä¸è€…è¡¨ä¼˜åŒ–ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_activity_participants_activity_user
ON activity_participants (activity_id, user_id)
WHERE status = 'CONFIRMED';

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_activity_participants_user_status
ON activity_participants (user_id)
WHERE status IN ('INVITED', 'CONFIRMED');

-- 3. å›¢é˜Ÿè¡¨å¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_teams_org_active
ON teams (organization_id, is_active)
WHERE is_active = true;

-- 4. é¢„ç®—è¡¨èŒƒå›´ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_budgets_activity_amount
ON budgets (activity_id, total_amount, used_amount)
WHERE status != 'DISABLED';

-- 5. ç”¨æˆ·è¡Œä¸ºè¿½è¸ªç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_activities_behavior
ON user_activities (user_id, activity_type, created_at DESC);

-- 6. å…¨æ–‡æœç´¢ä¸“ç”¨ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_activities_full_text
ON activities USING gin(to_tsvector('english',
    coalesce(title,'') || ' ' ||
    coalesce(location,'') || ' ' ||
    coalesce(description,'')
));
```

#### 2.3.3 å¤æ‚æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

```sql
-- å¤æ‚æŸ¥è¯¢ä¼˜åŒ–ç¤ºä¾‹
-- åŸå§‹æŸ¥è¯¢ï¼šè·å–å›¢é˜Ÿæ´»åŠ¨å’Œç»Ÿè®¡ä¿¡æ¯
EXPLAIN (ANALYZE, BUFFERS) WITH team_stats AS (
    SELECT
        t.id as team_id,
   count(a.id) as total_activities,
        avg(f.avg_rating) as avg_satisfaction_score
    FROM teams t
    LEFT JOIN activities a ON t.id = a.team_id AND a.status = 'COMPLETED'
LEFT JOIN feedbacks f ON a.id = f.activity_id
    WHERE t.organization_id = $uuid
      AND t.is_active = true
 GROUP BY t.id
),
recent_activities AS (
    SELECT
   t.id as team_id,
     a.id as activity_id,
        a.title,
   a.scheduled_date,
        a.participants_count,
   f.avg_rating
    FROM teams t
    JOIN activities a ON t.id = a.team_id
  LEFT JOIN (
        SELECT activity_id, AVG(rating) as avg_rating
   FROM feedbacks
     GROUP BY activity_id
    ) f ON a.id = f.activity_id
    WHERE t.organization_id = $uuid
      AND t.is_active = true
        AND a.created_at > NOW() - INTERVAL '90 days'
    ORDER BY a.created_at DESC
)
SELECT
    ts.*,
    json_agg(ra) FILTER (WHERE ra.activity_id IS NOT NULL) as recent_activities
FROM team_stats ts
LEFT JOIN recent_activities ra ON ts.team_id = ra.team_id
ORDER BY ts.avg_satisfaction_score DESC;

-- ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼ˆä½¿ç”¨CTEå’Œåˆé€‚çš„ç´¢å¼•ï¼‰
EXPLAIN (ANALYZE, BUFFERS) WITH team_stats AS (
    SELECT
        t.id as team_id,
 count(a.id) as total_activities,
    AVG(f.avg_rating) as avg_satisfaction_score
 FROM teams t
 LEFT JOIN LATERAL (
     SELECT a.id
     FROM activities a
            WHERE a.team_id = t.id
       AND a.status = 'COMPLETED'
   ) a ON true
    LEFT JOIN LATERAL (
     SELECT AVG(rating) as avg_rating
     FROM feedbacks f
            WHERE f.activity_id = a.id
        ) f ON true
    WHERE t.organization_id = $uuid
 AND t.is_active = true
    GROUP BY t.id
),
recent_activities AS (
    SELECT
     t.id as team_id,
        a.id as activity_id,
   a.title,
        a.scheduled_date,
        a.participants_count,
    f.avg_rating
    FROM teams t
    INNER JOIN activities a ON t.id = a.team_id
    LEFT JOIN LATERAL (
   SELECT AVG(rating) as avg_rating
            FROM feedbacks f
WHERE f.activity_id = a.id
          ) f ON true
 WHERE t.organization_id = $uuid
      AND t.is_active = true
        AND a.created_at > NOW() - INTERVAL '90 days'
    ORDER BY a.created_at DESC
  LIMIT 100  -- é™åˆ¶ç»“æœé›†å¤§å°
)
SELECT
    ts.*,
    coalesce(
  json_agg(json_build_object(
             'activity_id', ra.activity_id,
         'title', ra.title,
          'scheduled_date', ra.scheduled_date,
      'participants_count', ra.participants_count,
    'avg_rating', ra.avg_rating
            )) FILTER (WHERE ra.activity_id IS NOT NULL), '[]'
    ) as recent_activities
FROM team_stats ts
LEFT JOIN recent_activities ra ON ts.team_id = ra.team_id
ORDER BY COALESCE(ts.avg_satisfaction_score, 0) DESC;
```

## 3. Redisç¼“å­˜ä½“ç³»è®¾è®¡ (Redis Cache System Design)

### 3.1 ç¼“å­˜ç­–ç•¥è®¾è®¡

#### 3.1.1 å¤šçº§ç¼“å­˜æ¶æ„

```python
# ç¼“å­˜å±‚çº§å®šä¹‰
CACHE_LEVELS = {
    'L1': {
        'cache_type': 'local_caffeine',
        'ttl': 10,  # 10ç§’
   'max_size': 1000,
        'description': 'åº”ç”¨æœ¬åœ°ç¼“å­˜'
    },
    'L2': {
        'cache_type': 'redis_cluster',
        'ttl': 300,  # 5åˆ†é’Ÿ
        'max_size': 10000,
 'description': 'åˆ†å¸ƒå¼Redisç¼“å­˜'
    },
    'L3': {
        'cache_type': 'persisted_storage',
        'ttl': 86400,  # 24å°æ—¶
        'description': 'æŒä¹…åŒ–å­˜å‚¨'
    }
}

# ç¼“å­˜ç­–ç•¥åº”ç”¨
EXPIRATION_POLICIES = {
    'session_data': 1800,      # 30åˆ†é’Ÿ
    'activity_cache': 600,     # 10åˆ†é’Ÿ
    'ai_recommendation': 14400, # 4å°æ—¶
    'user_privileges': 3600,   # 1å°æ—¶
    'statistics_cache': 1800,  # 30åˆ†é’Ÿ
    'team_profile': 7200,      # 2å°æ—¶
}
```

#### 3.1.2 Redisé›†ç¾¤é…ç½®

```yaml
# Redis Clusteré…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-cluster-config
data:
  redis.conf: |
    # åŸºæœ¬è®¾ç½®
 port 6379
    tcp-backlog 511
timeout 30
tcp-keepalive 60
    loglevel notice
  logfile ""
  databases 16

    # å†…å­˜ç­–ç•¥
    maxmemory 512mb
maxmemory-policy allkeys-lru
maxmemory-samples 5

    # AOFç­–ç•¥
    appendonly yes
    appendfilename "appendonly.aof"
appendfsync everysec
    no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
    auto-aof-rewrite-min-size 64mb

# RDBç­–ç•¥
 save 900 1
    save 300 10
    save 60 10000
 stop-writes-on-bgsave-error yes
  rdbcompression yes
  rdbchecksum yes

    # é›†ç¾¤è®¾ç½®
    cluster-enabled yes
    cluster-config-file nodes.conf
    cluster-node-timeout 5000
   cluster-replica-validity-factor 10
    cluster-migration-barrier 1

    # æ…¢æŸ¥è¯¢æ—¥å¿—
    slowlog-log-slower-than 10000
    slowlog-max-len 128

    # é›†ç¾¤å®¢æˆ·ç«¯é…ç½®
    cluster-require-full-coverage no

    # å®‰å…¨è®¾ç½®
    # requirepass foobared
    # masterauth foobared

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-scripts
data:
  cache_functions.lua: |
    #!/bin/lua
    -- è‡ªå®šä¹‰Redisç¼“å­˜å‡½æ•°
    config
 -- ç¼“å­˜éªŒè¯ç å¹¶è®¾ç½®TTL
    function set_verification_code(code, ttl)
        local key = "verification:" .. tostring(code)
        redis.call('SET', key, 'valid', 'EX', ttl)
     return true
    end

    -- éªŒè¯éªŒè¯ç æ˜¯å¦æœ‰æ•ˆ
    function verify_code(code)
 local key = "verification:" .. tostring(code)
 local exists = redis.call('EXISTS', key)
 if exists == 1 then
   redis.call('DEL', key)  # éªŒè¯ååˆ é™¤
            return true
        end
     return false
    end

    -- åŸºäºæ»‘åŠ¨çª—å£çš„é™æµ
    function sliding_window_rate_limit(key_base, window_size, limit)
        local now = redis.call('TIME')[1]
local window = math.floor(now / window_size)
  local key = key_base..":"..window

     local current = redis.call('INCR', key)
        if current == 1 then
            redis.call('EXPIRE', key, window_size * 2)
   end

        if current > limit then
 return false
      end

    return true
    end
```

### 3.2 ç¼“å­˜æ€§èƒ½ä¼˜åŒ–

#### 3.2.1 ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–

```sql
-- ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§æŸ¥è¯¢
-- Redis INFOå‘½ä»¤ç›‘æ§åˆ†æ
WITH cache_stats AS (
    SELECT
        keyspace_hits,
    keyspace_misses,
   keyspace_hits + keyspace_misses as total_requests,
ROUND((keyspace_hits::float / (keyspace_hits + keyspace_misses)) * 100, 2) as hit_rate
    FROM redis_info
    WHERE timestamp > NOW() - INTERVAL '1 hour'
)
SELECT
  avg(hit_rate) as avg_cache_hit_rate,
    min(hit_rate) as min_hit_rate,
max(hit_rate) as max_hit_rate,
    CASE
WHEN avg(hit_rate) >= 90 THEN 'EXCELLENT'
      WHEN avg(hit_rate) >= 80 THEN 'GOOD'
      WHEN avg(hit_rate) >= 60 THEN 'FAIR'
    ELSE 'POOR'
    END as cache_performance_level
FROM cache_stats;
```

#### 3.2.2 çƒ­ç‚¹æ•°æ®è¯†åˆ«ä¸ä¼˜åŒ–

```sql
-- çƒ­ç‚¹é”®åˆ†ææŸ¥è¯¢
-- Rediså†…å­˜ä½¿ç”¨åˆ†æ
WITH key_memory_analysis AS (
    SELECT
        conn_id,
   db_id,
 key,
  ttl,
        size,
    encoding,
        lru_seconds_idle,
        CASE
            WHEN lru_seconds_idle < 60 THEN 'HOT_DATA'
            WHEN lru_seconds_idle < 300 THEN 'WARM_DATA'
          ELSE 'COLD_DATA'
        END AS access_pattern,
 CASE
WHEN size > 100000 THEN 'LARGE_OBJECT'
:when size > 10000 THEN 'MEDIUM_OBJECT'
       ELSE 'SMALL_OBJECT'
 END AS object_size_category
    FROM redis_memory_usage
    WHERE timestamp > NOW() - INTERVAL '1 hour'
)
SELECT
    access_pattern,
 object_size_category,
 count(*) as key_count,
    avg(size) as avg_size_bytes,
    sum(size) as total_memory_bytes,
    ROUND((sum(size) / (SELECT sum(size) FROM redis_memory_usage WHERE timestamp > NOW() - INTERVAL '1 hour')) * 100, 2) as memory_percentage
FROM key_memory_analysis
GROUP BY access_pattern, object_size_category
ORDER BY total_memory_bytes DESC;
```

## 4. æ•°æ®åˆ†æä¸ETL (Data Analytics & ETL)

### 4.1 æ•°æ®ä»“åº“è®¾è®¡

#### 4.1.1 æ˜Ÿå‹æ¶æ„è®¾è®¡

```sql
-- æ•°æ®ä»“åº“æ˜Ÿå‹æ¶æ„è®¾è®¡

-- äº‹å®è¡¨ï¼šæ´»åŠ¨äº‹å®
CREATE TABLE fact_activities (
    activity_sk BIGSERIAL PRIMARY KEY,
  activity_id UUID NOT NULL,
team_sk BIGINT NOT NULL,
    user_sk BIGINT NOT NULL,
    activity_type_sk INTEGER NOT NULL,
    status_sk INTEGER NOT NULL,
    budget_amount DECIMAL(12,2),
  participant_count INTEGER,
duration_minutes INTEGER,
     created_date_sk INTEGER NOT NULL,
 scheduled_date_sk INTEGER,
completed_date_sk INTEGER,
    satisfaction_score DECIMAL(3,1),
 feedback_count INTEGER,
insight_data JSONB,
 etl_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç»´åº¦è¡¨ï¼šå›¢é˜Ÿç»´åº¦CREATE TABLE dim_teams (
    team_sk BIGSERIAL PRIMARY KEY,
    team_id UUID NOT NULL,
    team_name VARCHAR(200) NOT NULL,
 organization_id UUID NOT NULL,
    organization_name VARCHAR(200) NOT NULL,
    member_count INTEGER NOT NULL,
    team_type VARCHAR(50),
    location_city VARCHAR(100),
  location_country VARCHAR(50),
    industry VARCHAR(100),
 is_active BOOLEAN DEFAULT TRUE,
    created_date DATE NOT NULL,
etl_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç»´åº¦è¡¨ï¼šç”¨æˆ·ç»´åº¦
CREATE TABLE dim_users (
    user_sk BIGSERIAL PRIMARY KEY,
    user_id UUID NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    full_name VARCHAR(255) NOT NULL,
    role VARCHAR(50),
    department VARCHAR(100),
    seniority_level VARCHAR(50),
  location_city VARCHAR(100),
    location_country VARCHAR(50),
    join_date DATE,
    last_login_date DATE,
    is_active BOOLEAN DEFAULT TRUE,
 etl_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç»´åº¦è¡¨ï¼šæ´»åŠ¨ç±»å‹ç»´åº¦
CREATE TABLE dim_activity_types (
 activity_type_sk SERIAL PRIMARY KEY,
    activity_type VARCHAR(50) UNIQUE NOT NULL,
    activity_category VARCHAR(50),
    min_participants INTEGER,
    max_participants INTEGER,
    typical_duration_minutes INTEGER,
    average_cost_low DECIMAL(10,2),
    average_cost_high DECIMAL(10,2),
    description TEXT,
 is_active BOOLEAN DEFAULT TRUE,
etl_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç»´åº¦è¡¨ï¼šæ—¥æœŸç»´åº¦
CREATE TABLE dim_dates (
    date_sk SERIAL PRIMARY KEY,
    date_actual DATE UNIQUE NOT NULL,
    year INTEGER NOT NULL,
    month INTEGER NOT NULL,
 month_name VARCHAR(20),
    month_name_short CHAR(3),
    quarter INTEGER NOT NULL,
  day_of_month INTEGER NOT NULL,
    day_of_year INTEGER NOT NULL,
day_of_week INTEGER NOT NULL,
    day_name VARCHAR(10),
is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    fiscal_year INTEGER,
    fiscal_quarter INTEGER,
    season VARCHAR(10)
);

-- å¡«å……æ—¥æœŸç»´åº¦æ•°æ®ï¼ˆç”Ÿæˆ3å¹´æ•°æ®ï¼‰
INSERT INTO dim_dates (
    date_actual, year, month, month_name, month_name_short, quarter,
  day_of_month, day_of_year, day_of_week, day_name, is_weekend,
 fiscal_year, fiscal_quarter, season
)
SELECT DATE '2020-01-01' + (i || ' days')::INTERVAL as date_actual,
       EXTRACT(YEAR FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as year,
 EXTRACT(MONTH FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as month,
   TO_CHAR(DATE '2020-01-01' + (i || ' days')::INTERVAL, 'Month') as month_name,
    TO_CHAR(DATE '2020-01-01' + (i || ' days')::INTERVAL, 'Mon') as month_name_short,
   EXTRACT(QUARTER FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as quarter,
   EXTRACT(DAY FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as day_of_month,
EXTRACT(DOY FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as day_of_year,
    EXTRACT(DOW FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as day_of_week,
TO_CHAR(DATE '2020-01-01' + (i || ' days')::INTERVAL, 'Day') as day_name,
    CASE WHEN EXTRACT(DOW FROM DATE '2020-01-01' + (i || ' days')::INTERVAL) IN (0,6) THEN true ELSE false END as is_weekend,
EXTRACT(YEAR FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as fiscal_year,
  EXTRACT(QUARTER FROM DATE '2020-01-01' + (i || ' days')::INTERVAL)::INT as fiscal_quarter,
     CASE
            WHEN EXTRACT(MONTH FROM DATE '2020-01-01' + (i || ' days')::INTERVAL) IN (12,1,2) THEN 'Winter'
    WHEN EXTRACT(MONTH FROM DATE '2020-01-01' + (i || ' days')::INTERVAL) IN (3,4,5) THEN 'Spring'
            WHEN EXTRACT(MONTH FROM DATE '2020-01-01' + (i || ' days')::INTERVAL) IN (6,7,8) THEN 'Summer'
            WHEN EXTRACT(MONTH FROM DATE '2020-01-01' + (i || ' days')::INTERVAL) IN (9,10,11) THEN 'Fall'
END as season
FROM generate_series(0, 1095) as i;  -- 3å¹´æ•°æ®
```

### 4.2 ETLæµç¨‹è®¾è®¡

#### 4.2.1 Airflow DAGé…ç½®

```python
# etl_dag.py - ETLæµç¨‹é…ç½®
import datetime
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.sensors.external_task_sensor import ExternalTaskSensor
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'dba-team',
    'depends_on_past': False,
    'start_date': days_ago(2),
    'email': ['dba@company.com'],
 'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# ä¸»æ•°æ®ETL DAG
dag = DAG(
    'team_building_analytics_etl',
    default_args=default_args,
    description='å›¢å»ºåŠ©æ‰‹åˆ†ææ•°æ®ETLæµç¨‹',
  schedule_interval='@daily',
    catchup=False,
    max_active_runs=1,
)

def extract_activity_data():
    """ä»ç”Ÿäº§æ•°æ®åº“æå–æ´»åŠ¨æ•°æ®"""
    import psycopg2
    import pandas as pd

    conn = psycopg2.connect(
        host='postgresql-primary.team-building.svc.cluster.local',
   database='team_building',
        user='etl_user',
        password='{{ var.value.postgres_password }}'
    )

    query = """
    SELECT
    a.id,
        a.team_id,
  a.title,
        a.status,
     a.type,
    a.budget_min,
        a.budget_max,
        a.created_at,
     a.scheduled_date,
        a.completed_date,
        ap.participant_count,
  f.avg_rating,
        f.feedback_count
    FROM activities a
  LEFT JOIN (
 SELECT activity_id, COUNT(*) as participant_count
FROM activity_participants
   WHERE status = 'CONFIRMED'
GROUP BY activity_id
) ap ON a.id = ap.activity_id
LEFT JOIN (
     SELECT activity_id, AVG(rating) as avg_rating, COUNT(*) as feedback_count
   FROM feedbacks
GROUP BY activity_id
    ) f ON a.id = f.activity_id
    WHERE a.created_at >= NOW() - INTERVAL '2 days'
    ORDER BY a.created_at DESC;
 """

    df = pd.read_sql(query, conn)
    conn.close()

    # ä¿å­˜åˆ°ä¸´æ—¶CSVæ–‡ä»¶
    csv_path = '/tmp/team_building_activities_{{ ds }}.csv'
    df.to_csv(csv_path, index=False)

    return csv_path

def transform_data():
    """æ•°æ®æ¸…æ´—å’Œè½¬æ¢"""
    import pandas as pd
    import numpy as np

    # è¯»å–æå–çš„æ•°æ®
    df = pd.read_csv('/tmp/team_building_activities_{{ ds }}.csv')

    # æ•°æ®æ¸…æ´—
    df['title_stemmed'] = df['title'].apply(lambda x: stem_text(str(x)))
    df['budget_range'] = (df['budget_max'] - df['budget_min']).astype(str)
    df['satisfaction_category'] = df['avg_rating'].apply(categorize_satisfaction)

    # æ•°æ®è´¨é‡æ£€æŸ¥
    if df.isnull().sum().sum() > 0:
        raise ValueError(f"å‘ç° {df.isnull().sum().sum()} ä¸ªç©ºå€¼ï¼Œéœ€è¦å¤„ç†")

    # æ•°æ®ä¸€è‡´æ€§éªŒè¯
    if len(df[df['participant_count'] > 500]) > 0:
        logging.warning(f"å‘ç° {len(df[df['participant_count'] > 500])} ä¸ªæ´»åŠ¨å‚ä¸è€…è¶…è¿‡500äºº")

    # ä¿å­˜è½¬æ¢åçš„æ•°æ®
    transformed_path = '/tmp/team_building_transformed_{{ ds }}.csv'
    df.to_csv(transformed_path, index=False)

    return transformed_path

def load_to_data_warehouse():
    """åŠ è½½æ•°æ®åˆ°åˆ†æä»“åº“"""
    import psycopg2
    import pandas as pd

    # è¿æ¥åˆ°åˆ†ææ•°æ®åº“
    conn = psycopg2.connect(
        host='postgresql-analytics.company.com',
        database='analytics_db',
        user='etl_loader',
        password='{{ var.value.analytics_db_password }}'
    )

    # è¯»å–è½¬æ¢åçš„æ•°æ®
    df = pd.read_csv('/tmp/team_building_transformed_{{ ds }}.csv')

    # æ•°æ®åŠ è½½é€»è¾‘ï¼ˆä½¿ç”¨COPYä¼˜é›…åœ°ä»é€Ÿåº¦)
    with conn.cursor() as cursor:
      # å¼€å§‹äº‹åŠ¡
   cursor.execute("BEGIN;")

    try:
       # åˆ é™¤ä»Šå¤©çš„é‡å¤æ•°æ®
 cursor.execute("""
                DELETE FROM fact_activities WHERE date_sk = (
   SELECT date_sk FROM dim_dates WHERE date_actual = %s
       )
        """, (datetime.now().date(),))

            # ä½¿ç”¨COPY FROM CSVåŠ è½½æ•°æ®
            buffer = StringIO()
            df.to_csv(buffer, index=False)
            buffer.seek(0)

        cursor.copy_expert("""
                COPY fact_activities (
  activity_id, team_sk, user_sk, activity_type_sk, status_sk,
                    budget_amount, participant_count, duration_minutes,
  created_date_sk, scheduled_date_sk, completed_date_sk,
      satisfaction_score, feedback_count, insight_data
  ) FROM STDIN WITH (FORMAT csv, HEADER true, DELIMITER ',')
            """, buffer)

 # æ›´æ–°æ•°æ®ä»“åº“æ¦‚è¦è¡¨
     cursor.execute("""
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_activities_daily;
           REFRESH MATERIALIZED VIEW CONCURRENTLY mv_team_performance;
   """)

    # æäº¤äº‹åŠ¡
   cursor.execute("COMMIT;")
        except Exception as e:
    # å›æ»šäº‹åŠ¡
       cursor.execute("ROLLBACK;")
    raise e
        finally:
      cursor.close()
            conn.close()

    return f"Successfully loaded {len(df)} records"

# å®šä¹‰ä»»åŠ¡
extract_task = PythonOperator(
    task_id='extract_activity_data',
    python_callable=extract_activity_data,
dag=dag,
)

transform_task = PythonOperator(
 task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_to_data_warehouse',
    python_callable=load_to_data_warehouse,
 dag=dag,
)

# å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
extract_task >> transform_task >> load_task
```

#### 4.2.2 å®æ—¶æ•°æ®æµå¤„ç†

```java
// å®æ—¶æ•°æ®æµå¤„ç†Kafkaæ¶ˆè´¹è€…
@Component
@Slf4j
@RequiredArgsConstructor
public class RealTimeAnalyticsProcessor {

    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final ElasticsearchRestTemplate elasticsearchTemplate;
    private final MeterRegistry meterRegistry;

    @KafkaListener(topics = "team-building-events", groupId = "analytics-processor")
    public void processActivityEvent(ActivityEvent event) {
        log.info("å¤„ç†æ´»åŠ¨äº‹ä»¶: {}", event.getEventType());

 StopWatch timer = StopWatch.createStarted();

        try {
         // æ ¹æ®äº‹ä»¶ç±»å‹å¤„ç†ä¸åŒåˆ†æé€»è¾‘
     switch (event.getEventType()) {
         case "ACTIVITY_CREATED":
       processActivityCreated(event);
        break;
case "ACTIVITY_COMPLETED":
                processActivityCompleted(event);
                break;
         case "USER_PARTICIPATED":
       processUserParticipation(event);
      break;
 case "FEEDBACK_SUBMITTED":
              processFeedbackSubmitted(event);
      break;
         default:
           log.warn("æœªçŸ¥äº‹ä»¶ç±»å‹: {}", event.getEventType());
            }

            // è®°å½•å¤„ç†æŒ‡æ ‡
            timer.stop();
         meterRegistry.timer("analytics.event.process.time", "type", event.getEventType())
     .record(timer.getTime(TimeUnit.MILLISECONDS), TimeUnit.MILLISECONDS);

        } catch (Exception e) {
   log.error("å¤„ç†æ´»åŠ¨äº‹ä»¶å¤±è´¥: {}", event.getId(), e);
            // å‘é€åˆ°é”™è¯¯é˜Ÿåˆ—
   kafkaTemplate.send("analytics-error", event);
    }
    }

    private void processActivityCreated(ActivityEvent event) {
        // å®æ—¶æ›´æ–°æ´»åŠ¨åˆ›å»ºæŒ‡æ ‡
     meterRegistry.counter("analytics.activities.created",
          "type", event.getActivityType())
         .increment();

      // æ›´æ–°å®æ—¶çœ‹æ¿æ•°æ®
      elasticsearchTemplate.update(UpdateQuery.builder(event.getActivityId())
    .withScript(new Script(ELASTIC_UPDATE_SCRIPT))
.withParams(Map.of(
     "created_count", 1,
   "created_timestamp", Instant.now()
   ))
      .build(), ActivityMetrics.class);
    }

    private void processActivityCompleted(ActivityEvent event) {
  // è®¡ç®—æ´»åŠ¨KPI
        ActivityKPI kpi = calculateActivityKPI(event.getActivityId());

        // å‘é€åˆ°Kafkaè¿›è¡Œæ·±åº¦åˆ†æ
        kafkaTemplate.send("activity-analytics-deep",
  objectMapper.writeValueAsString(kpi));

        // æ›´æ–°å›¢é˜Ÿå‚ä¸åº¦æŒ‡æ ‡
        elasticsearchTemplate.update(
            Query.query(Criteria.where("team_id").is(event.getTeamId())),
   Update.update("$inc", Map.of("completed_count", 1))
       .set("last_completed_date", event.getTimestamp()),
  TeamMetrics.class
     );
    }

    private void processFeedbackSubmitted(ActivityEvent event) {
        // å®æ—¶æ›´æ–°æ»¡æ„åº¦æŒ‡æ ‡
     SatisfactionMetrics metrics = calculateSatisfactionMetrics(
   event.getActivityId(),
            event.getUserId(),
    event.getSatisfactionScore()
     );

// å‘é€åˆ°æœºå™¨å­¦ä¹ æœåŠ¡è¿›è¡Œæ¨¡å‹è®­ç»ƒ
kafkaTemplate.send("feedback-ml-training",
         objectMapper.writeValueAsString(buildMLTrainingData(metrics)));
    }
}
```

## 5. æ•°æ®å¤‡ä»½ä¸æ¢å¤ç­–ç•¥ (Backup & Recovery)

### 5.1 åˆ†å±‚å¤‡ä»½ä½“ç³»

#### 5.1.1 æ•°æ®åº“å¤‡ä»½ç±»å‹

```bash
#!/bin/bash
# backup-manager.sh - æ•°æ®åº“å¤‡ä»½ç®¡ç†ç³»ç»Ÿ

set -euo pipefail

echo "ğŸš€ å›¢å»ºåŠ©æ‰‹æ•°æ®åº“å¤‡ä»½ç®¡ç†ç³»ç»Ÿå¯åŠ¨"

# åŸºç¡€é…ç½®
BACKUP_ROOT="/mnt/backup/team-building"
S3_DESTINATION="s3://teambuilding-db-backups"
RETENTION_DAYS=90

# åˆ›å»ºå¤‡ä»½ç›®å½•ç»“æ„
mkdir -p "$BACKUP_ROOT"/{full,incremental,archive,wal,spatio_tempore}

# å‡½æ•°ï¼šåˆ›å»ºPostgreSQLå…¨é‡å¤‡ä»½
function create_postgres_full_backup() {
    local backup_date=$(date +%Y%m%d_%H%M%S)
   local backup_path="$BACKUP_ROOT/full/postgres_${backup_date}"

    echo "å¼€å§‹ PostgreSQL å…¨é‡å¤‡ä»½: $backup_date"

    # åœ¨ä¸»èŠ‚ç‚¹ä¸Šæ‰§è¡ŒåŸºå‡†å¤‡ä»½
    pg_basebackup \
        -h postgresql-primary.team-building.svc.cluster.local \
    -U backup_user \
   -D "$backup_path" \
     progress \
  -Ft -z \
        -X stream \
   -c fast \
     --checkpoint=fast \
 -v \
  --slot=backup_slot_$(hostname) || {
      echo "å…¨é‡å¤‡ä»½å¤±è´¥"
  return 1
    }

    # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
    if validate_backup "$backup_path"; then
        echo "âœ… PostgreSQL å…¨é‡å¤‡ä»½éªŒè¯é€šè¿‡"
    else
        echo "âŒ PostgreSQL å…¨é‡å¤‡ä»½éªŒè¯å¤±è´¥"
      return 1
    fi

    # ä¸Šä¼ åˆ°S3ï¼ˆä¸åŒå­˜å‚¨çº§åˆ«ï¼‰
    echo "ä¸Šä¼ åˆ°S3..."
    aws s3 cp "$backup_path" "$S3_DESTINATION/full/postgres_${backup_date}/" \
        --recursive \\
        --storage-class GLACIER \\
        --metadata "backup-date=$backup_date,backup-type=full"

# æœ¬åœ°ä¿ç•™æœ€æ–°3ä»½
    find "$BACKUP_ROOT/full" -maxdepth 1 -type d -name "postgres_*" -mtime +7 | head -n -3 | xargs rm -rf

    echo "âœ… PostgreSQL å…¨é‡å¤‡ä»½å®Œæˆ: $backup_date"
    echo "å¤‡ä»½å¤§å°: $(du -sh "$backup_path" | awk '{print $1}')"
}

# å‡½æ•°ï¼šåˆ›å»ºPostgreSQLå¢é‡å¤‡ä»½ï¼ˆbased on WALï¼‰
function create_postgres_incremental_backup() {
    local backup_date=$(date +%Y%m%d_%H%M%S)
    local backup_path="$BACKUP_ROOT/incremental/postgres_wal_${backup_date}"

    echo "å¼€å§‹ PostgreSQL WAL å¢é‡å¤‡ä»½: $backup_date"

    mkdir -p "$backup_path"

    # ä½¿ç”¨pg_receivewalè¿›è¡Œå¢é‡WALå¤‡ä»½
    pg_receivewal \
        -h postgresql-primary.team-building.svc.cluster.local \
   -U wal_backup \
        -D "$backup_path" \
        -S backup_slot_wal_$(hostname) \
        -n 2 \
    -v || {
     echo "å¢é‡å¤‡ä»½å¤±è´¥"
            return 1
        }

    # ä¸Šä¼ åˆ°S3ï¼ˆä½¿ç”¨æ ‡å‡†-IAå­˜å‚¨ï¼‰(ä½¿ç”¨æ ‡å‡†-IAå­˜å‚¨ï¼‰
    aws s3 sync "$backup_path" "$S3_DESTINATION/incremental/postgres_wal_${backup_date}/" \
        --storage-class STANDARD_IA

    # ç”Ÿæˆå¤‡ä»½æ¸…å•
    cat > "$backup_path/backup_manifest.json" <<EOF
{
  "backup_type": "wal_incremental",
  "backup_date": "${backup_date}",
  "backup_slot": "backup_slot_wal_$(hostname)",
  "wal_files_count": $(ls -1 "$backup_path"/*.partial 2>/dev/null | wc -l),
  "backup_size_bytes": $(du -sb "$backup_path" | awk '{print $1}'),
  "backup_md5": "$(find "$backup_path" -name "*.partial" -exec md5sum {} \; | md5sum | awk '{print $1}')"
}
EOF

  echo "âœ… PostgreSQL å¢é‡å¤‡ä»½å®Œæˆ: $backup_date"
}

# å‡½æ•°ï¼šéªŒè¯å¤‡ä»½å®Œæ•´æ€§
function validate_backup() {
    local backup_path=$1
    local result=0

    echo "éªŒè¯å¤‡ä»½å®Œæ•´æ€§..."

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [[ ! -d "$backup_path" ]]; then
  echo "å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: $backup_path"
 return 1
    fi

# è®¡ç®—å¤‡ä»½å¤§å°
    local size=$(du -sb "$backup_path" 2>/dev/null | awk '{print $1}')
    if [[ $size -eq 0 ]]; then
        echo "å¤‡ä»½å†…å®¹ä¸ºç©º"
        return 1
    fi

    # æ£€æŸ¥PostgreSQLå¤‡ä»½ç‰¹å®šçš„æ–‡ä»¶
    if [[ -f "$backup_path/base.tar.gz" ]]; then
        # ä»¥è§£å‹æµ‹è¯•
        if ! tar -tzf "$backup_path/base.tar.gz